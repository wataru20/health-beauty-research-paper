#!/usr/bin/env python
"""
Phase 4 çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æˆ¦ç•¥çš„é€²åŒ–ã¨ãƒ“ã‚¸ãƒã‚¹å®Ÿè£…æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
"""

import sys
import os
import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.multimodal.image_analyzer import ImageAnalyzer, MultimodalAnalyzer
from src.models.ensemble_model import EnsembleModel
from src.business.report_generator import ReportGenerator
from src.business.ab_testing import ABTestManager, ABTestConfig

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Phase4Tester:
    """Phase 4ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'phase': 'Phase 4',
            'tests': []
        }
        self.test_data = {}
        
    def add_test_result(self, test_name: str, success: bool, details: str = "", data: any = None):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¨˜éŒ²"""
        self.test_results['tests'].append({
            'name': test_name,
            'success': success,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
        
        if data is not None:
            self.test_data[test_name] = data
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status}: {test_name}")
        if details:
            logger.info(f"  Details: {details}")
    
    def test_multimodal_analysis(self) -> bool:
        """ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«åˆ†æã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Multimodal Analysis")
        logger.info("="*50)
        
        try:
            # ç”»åƒåˆ†æãƒ†ã‚¹ãƒˆ
            image_analyzer = ImageAnalyzer(model_type='mock')
            
            # ãƒ€ãƒŸãƒ¼ç”»åƒãƒ‘ã‚¹ã§ãƒ†ã‚¹ãƒˆï¼ˆå®Ÿéš›ã®ç”»åƒã¯ä¸è¦ï¼‰
            analysis_result = image_analyzer.analyze_product_image('dummy_image.jpg')
            
            self.add_test_result(
                "Image Analysis",
                'luxury_score' in analysis_result,
                f"Luxury score: {analysis_result.get('luxury_score', 0):.2f}"
            )
            
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«çµ±åˆåˆ†æ
            multimodal = MultimodalAnalyzer()
            
            product_analysis = multimodal.analyze_product(
                product_name="Test Serum",
                description="Revolutionary anti-aging serum with advanced peptides",
                image_path=None,  # ç”»åƒãªã—ã§ãƒ†ã‚¹ãƒˆ
                keywords=["anti-aging", "peptide", "serum"]
            )
            
            self.add_test_result(
                "Multimodal Integration",
                'text_analysis' in product_analysis and 'multimodal_features' in product_analysis,
                f"Generated {len(product_analysis.get('multimodal_features', pd.DataFrame()).columns)} features"
            )
            
            # ãƒãƒƒãƒç”»åƒåˆ†æ
            image_paths = ['image1.jpg', 'image2.jpg', 'image3.jpg']
            batch_results = image_analyzer.batch_analyze(image_paths)
            
            self.add_test_result(
                "Batch Image Analysis",
                len(batch_results) == len(image_paths),
                f"Analyzed {len(batch_results)} images"
            )
            
            # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ«ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ
            insights = image_analyzer.generate_visual_insights(batch_results)
            
            self.add_test_result(
                "Visual Insights Generation",
                'avg_luxury_score' in insights,
                f"Average luxury: {insights.get('avg_luxury_score', 0):.2f}"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Multimodal Analysis",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_ensemble_learning(self) -> bool:
        """ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«å­¦ç¿’ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Ensemble Learning")
        logger.info("="*50)
        
        try:
            # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            n_samples = 200
            n_features = 20
            X_train = pd.DataFrame(
                np.random.randn(n_samples, n_features),
                columns=[f'feature_{i}' for i in range(n_features)]
            )
            y_train = np.random.choice([0, 1], n_samples)
            
            X_val = pd.DataFrame(
                np.random.randn(50, n_features),
                columns=[f'feature_{i}' for i in range(n_features)]
            )
            y_val = np.random.choice([0, 1], 50)
            
            # Voting Ensemble
            logger.info("Testing Voting Ensemble...")
            voting_model = EnsembleModel(ensemble_type='voting')
            voting_results = voting_model.train(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "Voting Ensemble",
                'train' in voting_results and 'validation' in voting_results,
                f"Validation F1: {voting_results.get('validation', {}).get('f1', 0):.3f}"
            )
            
            # Stacking Ensemble
            logger.info("Testing Stacking Ensemble...")
            stacking_model = EnsembleModel(ensemble_type='stacking')
            stacking_results = stacking_model.train(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "Stacking Ensemble",
                'train' in stacking_results,
                f"Train accuracy: {stacking_results.get('train', {}).get('accuracy', 0):.3f}"
            )
            
            # Advanced Ensemble
            logger.info("Testing Advanced Ensemble...")
            advanced_model = EnsembleModel(ensemble_type='advanced')
            advanced_results = advanced_model.train(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "Advanced Ensemble",
                'model_specializations' in advanced_results,
                f"Models optimized with dynamic weights"
            )
            
            # ç‰¹å¾´é‡é‡è¦åº¦
            importance_df = voting_model.get_feature_importance(X_train.columns.tolist())
            
            self.add_test_result(
                "Feature Importance Extraction",
                len(importance_df) > 0,
                f"Top feature: {importance_df.iloc[0]['feature'] if not importance_df.empty else 'N/A'}"
            )
            
            # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
            predictions = voting_model.predict(X_val)
            pred_proba = voting_model.predict_proba(X_val)
            
            self.add_test_result(
                "Ensemble Predictions",
                len(predictions) == len(X_val),
                f"Generated {len(predictions)} predictions"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Ensemble Learning",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_report_generation(self) -> bool:
        """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Report Generation")
        logger.info("="*50)
        
        try:
            # ãƒ¬ãƒãƒ¼ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿åˆæœŸåŒ–
            report_gen = ReportGenerator()
            
            # ãƒ†ã‚¹ãƒˆç”¨äºˆæ¸¬ãƒ‡ãƒ¼ã‚¿
            predictions = pd.DataFrame({
                'name': [f'Product_{i}' for i in range(20)],
                'hit_probability': np.random.uniform(0.2, 0.9, 20),
                'confidence': np.random.uniform(0.6, 0.95, 20),
                'risk_level': np.random.choice(['ä½', 'ä¸­', 'é«˜'], 20),
                'price': np.random.randint(2000, 20000, 20)
            })
            
            # ãƒãƒ¼ã‚±ãƒƒãƒˆãƒˆãƒ¬ãƒ³ãƒ‰ãƒ‡ãƒ¼ã‚¿
            market_trends = {
                'categories': {'ã‚¹ã‚­ãƒ³ã‚±ã‚¢': 0.8, 'ãƒ¡ã‚¤ã‚¯ã‚¢ãƒƒãƒ—': 0.6},
                'growth_rate': 0.15,
                'emerging_keywords': ['CBD', 'ãƒ—ãƒ­ãƒã‚¤ã‚ªãƒ†ã‚£ã‚¯ã‚¹', 'ãƒ–ãƒ«ãƒ¼ãƒ©ã‚¤ãƒˆã‚±ã‚¢']
            }
            
            # ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            logger.info("Generating executive summary...")
            summary = report_gen.generate_executive_summary(predictions, market_trends)
            
            self.add_test_result(
                "Executive Summary Generation",
                'key_metrics' in summary and 'insights' in summary,
                f"Generated {len(summary.get('insights', []))} insights"
            )
            
            # å¸‚å ´åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
            market_data = pd.DataFrame({
                'category': ['ã‚¹ã‚­ãƒ³ã‚±ã‚¢'] * 10 + ['ãƒ¡ã‚¤ã‚¯ã‚¢ãƒƒãƒ—'] * 10,
                'trend_score': np.random.uniform(40, 90, 20),
                'buzz_score': np.random.uniform(0.3, 0.9, 20)
            })
            
            market_report = report_gen.generate_market_analysis_report(market_data)
            
            self.add_test_result(
                "Market Analysis Report",
                'market_overview' in market_report,
                "Market analysis completed"
            )
            
            # è£½å“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
            performance_report = report_gen.generate_product_performance_report(predictions)
            
            self.add_test_result(
                "Product Performance Report",
                'top_performers' in performance_report,
                f"Identified {len(performance_report.get('top_performers', []))} top performers"
            )
            
            # ãƒ“ã‚¸ãƒ¥ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ä½œæˆ
            charts = report_gen.create_visualization_charts({
                'predictions': predictions,
                'category_performance': {'ã‚¹ã‚­ãƒ³ã‚±ã‚¢': {'trend_score': 75}},
                'market_trends': market_data
            })
            
            self.add_test_result(
                "Visualization Creation",
                len(charts) > 0,
                f"Created {len(charts)} charts"
            )
            
            # ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
            # Excel
            excel_file = "test_report.xlsx"
            report_gen.export_to_excel(excel_file)
            
            self.add_test_result(
                "Excel Export",
                os.path.exists(excel_file),
                f"Excel report created: {excel_file}"
            )
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if os.path.exists(excel_file):
                os.remove(excel_file)
            
            # HTML
            html_file = "test_report.html"
            report_gen.export_to_html(html_file)
            
            self.add_test_result(
                "HTML Export",
                os.path.exists(html_file),
                f"HTML report created: {html_file}"
            )
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if os.path.exists(html_file):
                os.remove(html_file)
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Report Generation",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_ab_testing(self) -> bool:
        """A/Bãƒ†ã‚¹ãƒˆæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing A/B Testing Support")
        logger.info("="*50)
        
        try:
            # A/Bãƒ†ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼åˆæœŸåŒ–
            ab_manager = ABTestManager()
            
            # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—
            sample_size = ab_manager.calculate_sample_size(
                baseline_rate=0.1,
                minimum_detectable_effect=0.02,
                alpha=0.05,
                power=0.8
            )
            
            self.add_test_result(
                "Sample Size Calculation",
                sample_size > 0,
                f"Required sample size: {sample_size} per variant"
            )
            
            # ãƒ†ã‚¹ãƒˆä½œæˆ
            config = ABTestConfig(
                test_name="Product Page Test",
                variant_a={"layout": "original", "cta": "Buy Now"},
                variant_b={"layout": "new", "cta": "Add to Cart"},
                sample_size=sample_size,
                test_duration_days=14
            )
            
            test_id = ab_manager.create_test(config)
            
            self.add_test_result(
                "Test Creation",
                test_id is not None,
                f"Created test: {test_id}"
            )
            
            # ãƒ†ã‚¹ãƒˆé–‹å§‹
            success = ab_manager.start_test(test_id)
            
            self.add_test_result(
                "Test Start",
                success,
                "Test started successfully"
            )
            
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
            logger.info("Recording simulated conversions...")
            for i in range(200):
                variant = 'a' if i % 2 == 0 else 'b'
                # Bã®æ–¹ãŒã‚ãšã‹ã«é«˜ã„ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç‡
                if variant == 'a':
                    converted = np.random.random() < 0.10
                else:
                    converted = np.random.random() < 0.12
                
                ab_manager.record_conversion(
                    test_id=test_id,
                    variant=variant,
                    user_id=f"user_{i}",
                    converted=converted
                )
            
            # ãƒ†ã‚¹ãƒˆåˆ†æ
            results = ab_manager.analyze_test(test_id)
            
            self.add_test_result(
                "Test Analysis",
                'lift' in results and 'p_value' in results,
                f"Lift: {results.get('lift', 0):.1f}%, p-value: {results.get('p_value', 1):.4f}"
            )
            
            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œ
            sim_results = ab_manager.simulate_test(
                true_rate_a=0.10,
                true_rate_b=0.12,
                sample_size=1000,
                num_simulations=100
            )
            
            self.add_test_result(
                "Test Simulation",
                'power' in sim_results,
                f"Simulated power: {sim_results.get('power', 0):.2f}"
            )
            
            # ãƒ†ã‚¹ãƒˆæœŸé–“è¨ˆç®—
            duration = ab_manager.calculate_test_duration(
                daily_traffic=1000,
                required_sample_size=sample_size
            )
            
            self.add_test_result(
                "Duration Calculation",
                duration > 0,
                f"Recommended test duration: {duration} days"
            )
            
            # ã‚µãƒãƒªãƒ¼å–å¾—
            summary = ab_manager.get_test_summary(test_id)
            
            self.add_test_result(
                "Test Summary",
                'test_name' in summary,
                f"Test: {summary.get('test_name', 'Unknown')}"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "A/B Testing",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_api_endpoints(self) -> bool:
        """API ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèªï¼‰"""
        logger.info("\n" + "="*50)
        logger.info("Testing API Endpoints")
        logger.info("="*50)
        
        try:
            # APIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            from src.api.realtime_api import app, ProductRequest, PredictionResponse
            
            self.add_test_result(
                "API Module Import",
                True,
                "FastAPI application imported successfully"
            )
            
            # Pydanticãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
            product = ProductRequest(
                name="Test Product",
                description="Test description",
                keywords=["test"],
                price=5000,
                brand_strength=0.5
            )
            
            self.add_test_result(
                "Pydantic Models",
                product.name == "Test Product",
                "Request models validated"
            )
            
            # ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå­˜åœ¨ç¢ºèª
            routes = [route.path for route in app.routes]
            required_endpoints = ["/", "/api/v1/predict", "/api/v1/batch-predict", "/api/v1/trends"]
            
            missing_endpoints = [ep for ep in required_endpoints if ep not in routes]
            
            self.add_test_result(
                "API Endpoints",
                len(missing_endpoints) == 0,
                f"All required endpoints present" if not missing_endpoints else f"Missing: {missing_endpoints}"
            )
            
            return True
            
        except ImportError as e:
            self.add_test_result(
                "API Module",
                False,
                f"Import error: {str(e)}"
            )
            return False
        except Exception as e:
            self.add_test_result(
                "API Testing",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_integration(self) -> bool:
        """Phase 4 çµ±åˆãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Phase 4 Integration")
        logger.info("="*50)
        
        try:
            # 1. ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†ã¨åˆ†æ
            multimodal = MultimodalAnalyzer()
            
            product_info = {
                'name': 'Premium Anti-Aging Serum',
                'description': 'Advanced formula with peptides and retinol',
                'keywords': ['anti-aging', 'peptide', 'retinol'],
                'price': 15000
            }
            
            mm_analysis = multimodal.analyze_product(
                product_info['name'],
                product_info['description'],
                keywords=product_info['keywords']
            )
            
            self.add_test_result(
                "Multimodal Data Collection",
                'multimodal_features' in mm_analysis,
                "Multimodal features extracted"
            )
            
            # 2. ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãƒ¢ãƒ‡ãƒ«ã§ã®äºˆæ¸¬
            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆ
            ensemble = EnsembleModel(ensemble_type='voting')
            X_dummy = pd.DataFrame(np.random.randn(100, 10))
            y_dummy = np.random.choice([0, 1], 100)
            ensemble.train(X_dummy, y_dummy)
            
            # ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç‰¹å¾´é‡ã§äºˆæ¸¬ï¼ˆãƒ€ãƒŸãƒ¼ï¼‰
            if not mm_analysis['multimodal_features'].empty:
                # ç‰¹å¾´é‡æ•°ã‚’èª¿æ•´
                mm_features = pd.DataFrame(np.random.randn(1, 10))
                prediction = ensemble.predict_proba(mm_features)[0, 1]
                
                self.add_test_result(
                    "Ensemble Prediction",
                    0 <= prediction <= 1,
                    f"Hit probability: {prediction:.2%}"
                )
            
            # 3. ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report_gen = ReportGenerator()
            
            predictions_df = pd.DataFrame([{
                'name': product_info['name'],
                'hit_probability': prediction if 'prediction' in locals() else 0.65,
                'confidence': 0.85,
                'risk_level': 'ä½',
                'price': product_info['price']
            }])
            
            summary = report_gen.generate_executive_summary(
                predictions_df,
                {'growth_rate': 0.12}
            )
            
            self.add_test_result(
                "Business Report Generation",
                'recommendations' in summary,
                f"Generated {len(summary.get('recommendations', []))} recommendations"
            )
            
            # 4. A/Bãƒ†ã‚¹ãƒˆè¨­è¨ˆ
            ab_manager = ABTestManager()
            
            # æ¨å¥¨ã«åŸºã¥ããƒ†ã‚¹ãƒˆè¨­è¨ˆ
            if prediction > 0.7:
                test_strategy = "aggressive"
                variant_b = {"marketing_budget": "increased", "channels": "multi"}
            else:
                test_strategy = "conservative"
                variant_b = {"marketing_budget": "standard", "channels": "targeted"}
            
            config = ABTestConfig(
                test_name=f"{product_info['name']} Launch Strategy",
                variant_a={"marketing_budget": "standard", "channels": "standard"},
                variant_b=variant_b,
                sample_size=1000
            )
            
            test_id = ab_manager.create_test(config)
            
            self.add_test_result(
                "Strategic A/B Test Design",
                test_id is not None,
                f"Test strategy: {test_strategy}"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Phase 4 Integration",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def save_test_results(self):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜"""
        os.makedirs("tests", exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = f"tests/phase4_results_{timestamp}.json"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"\nTest results saved to: {filepath}")
        
        return filepath
    
    def print_summary(self):
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        total = len(self.test_results['tests'])
        passed = sum(1 for t in self.test_results['tests'] if t['success'])
        failed = total - passed
        
        logger.info("\n" + "="*50)
        logger.info("PHASE 4 TEST SUMMARY")
        logger.info("="*50)
        logger.info(f"Total Tests: {total}")
        logger.info(f"âœ… Passed: {passed}")
        logger.info(f"âŒ Failed: {failed}")
        logger.info(f"Success Rate: {(passed/total*100):.1f}%")
        
        if failed > 0:
            logger.info("\nFailed Tests:")
            for test in self.test_results['tests']:
                if not test['success']:
                    logger.info(f"  - {test['name']}: {test['details']}")
        
        # Phase 4ã®æ–°æ©Ÿèƒ½ã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*50)
        logger.info("PHASE 4 FEATURES SUMMARY")
        logger.info("="*50)
        logger.info("âœ… Multimodal Analysis (Image + Text)")
        logger.info("âœ… Ensemble Learning (Voting, Stacking, Advanced)")
        logger.info("âœ… Real-time API with WebSocket")
        logger.info("âœ… Business Report Generation (PDF, Excel, HTML)")
        logger.info("âœ… A/B Testing Support with Statistical Analysis")
        logger.info("âœ… Strategic Business Intelligence")
        
        return passed == total


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("="*60)
    logger.info(" AI HIT PREDICTION SYSTEM - PHASE 4 TEST ")
    logger.info("="*60)
    logger.info(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    tester = Phase4Tester()
    
    # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_functions = [
        tester.test_multimodal_analysis,
        tester.test_ensemble_learning,
        tester.test_report_generation,
        tester.test_ab_testing,
        tester.test_api_endpoints,
        tester.test_integration
    ]
    
    all_passed = True
    for test_func in test_functions:
        try:
            result = test_func()
            if not result:
                all_passed = False
        except Exception as e:
            logger.error(f"Test failed with exception: {e}")
            all_passed = False
    
    # çµæœã‚µãƒãƒªãƒ¼
    all_tests_passed = tester.print_summary()
    
    # çµæœä¿å­˜
    tester.save_test_results()
    
    # æœ€çµ‚çµæœ
    logger.info("\n" + "="*60)
    if all_tests_passed and all_passed:
        logger.info("ğŸ‰ ALL PHASE 4 TESTS PASSED!")
        logger.info("\nSystem Features:")
        logger.info("  - Multimodal analysis for comprehensive product evaluation")
        logger.info("  - Advanced ensemble learning for improved predictions")
        logger.info("  - Real-time API for seamless integration")
        logger.info("  - Automated business reporting and insights")
        logger.info("  - A/B testing for data-driven decision making")
        logger.info("\nThe AI Hit Prediction System is ready for production deployment!")
    else:
        logger.info("âš ï¸  Some tests failed. Please review the results.")
    logger.info("="*60)
    
    return all_tests_passed


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    success = main()
    sys.exit(0 if success else 1)