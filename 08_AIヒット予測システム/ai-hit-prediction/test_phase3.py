#!/usr/bin/env python
"""
Phase 3 çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
SHAPåˆ†æã€Streamlit UIã€Optunaæœ€é©åŒ–ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®å‹•ä½œç¢ºèª
"""

import sys
import os
import pandas as pd
import numpy as np
import logging
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from src.preprocessing.data_pipeline import DataPipeline
from src.preprocessing.feature_engineering import FeatureEngineer
from src.models.basic_model import HitPredictionModel
from src.analysis.model_explainer import ModelExplainer
from src.optimization.hyperparameter_optimizer import HyperparameterOptimizer, AutoML

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class Phase3Tester:
    """Phase 3ã®çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.test_results = {
            'timestamp': datetime.now().isoformat(),
            'phase': 'Phase 3',
            'tests': []
        }
        self.test_data = {}
        self.sample_data = None
        self.model = None
        
    def add_test_result(self, test_name: str, success: bool, details: str = "", data: any = None):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’è¨˜éŒ²"""
        self.test_results['tests'].append({
            'name': test_name,
            'success': success,
            'details': details,
            'timestamp': datetime.now().isoformat()
        })
        
        if data is not None:
            self.test_data[test_name] = data
        
        status = "âœ… PASS" if success else "âŒ FAIL"
        logger.info(f"{status}: {test_name}")
        if details:
            logger.info(f"  Details: {details}")
    
    def prepare_test_data(self):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™"""
        logger.info("\n" + "="*50)
        logger.info("Preparing Test Data")
        logger.info("="*50)
        
        try:
            # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã¨ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢åˆæœŸåŒ–
            pipeline = DataPipeline()
            engineer = FeatureEngineer()
            
            # ã‚µãƒ³ãƒ—ãƒ«è£½å“ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            products = []
            for i in range(100):
                products.append({
                    'name': f'Test Product {i}',
                    'keywords': ['test', 'sample', 'product'],
                    'price': np.random.randint(2000, 20000),
                    'brand_strength': np.random.uniform(0.3, 0.9),
                    'ingredient_novelty': np.random.uniform(0.2, 0.8),
                    'market_saturation': np.random.uniform(0.1, 0.6)
                })
            
            # ç‰¹å¾´é‡æŠ½å‡º
            X_list = []
            for product in products:
                features = pipeline.extract_features(product)
                X_list.append(features)
            
            X = pd.concat(X_list, ignore_index=True)
            X_enhanced = engineer.create_advanced_features(X)
            
            # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆç”Ÿæˆï¼ˆãƒ¢ãƒƒã‚¯ï¼‰
            y = np.random.choice([0, 1], size=len(products), p=[0.6, 0.4])
            
            # è¨“ç·´ãƒ»æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
            split_idx = int(len(X_enhanced) * 0.8)
            X_train = X_enhanced[:split_idx]
            y_train = y[:split_idx]
            X_val = X_enhanced[split_idx:]
            y_val = y[split_idx:]
            
            self.sample_data = {
                'X_train': X_train,
                'y_train': y_train,
                'X_val': X_val,
                'y_val': y_val,
                'products': products
            }
            
            self.add_test_result(
                "Test Data Preparation",
                True,
                f"Prepared {len(X_train)} training and {len(X_val)} validation samples"
            )
            
            # åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
            model = HitPredictionModel()
            model.train(X_train, y_train, validate=False)
            self.model = model
            
            self.add_test_result(
                "Base Model Training",
                True,
                "Model trained successfully for testing"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Test Data Preparation",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_shap_analysis(self) -> bool:
        """SHAPåˆ†ææ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing SHAP Analysis Module")
        logger.info("="*50)
        
        try:
            if self.model is None or self.sample_data is None:
                raise ValueError("Test data not prepared")
            
            # ModelExplaineråˆæœŸåŒ–
            explainer = ModelExplainer(self.model.model)
            
            # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ«ã®èª¬æ˜
            X_single = self.sample_data['X_val'].iloc[[0]]
            
            # SHAPå€¤è¨ˆç®—
            logger.info("Calculating SHAP values...")
            shap_values = explainer.calculate_shap_values(X_single)
            
            self.add_test_result(
                "SHAP Values Calculation",
                shap_values is not None and shap_values.shape == X_single.shape,
                f"SHAP values shape: {shap_values.shape}"
            )
            
            # ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
            importance = explainer.get_feature_importance()
            self.add_test_result(
                "Feature Importance Extraction",
                len(importance) > 0,
                f"Top 3 features: {list(importance.head(3).index)}"
            )
            
            # Force plot ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            force_plot_data = explainer.create_force_plot_data(X_single)
            self.add_test_result(
                "Force Plot Data Generation",
                'base_value' in force_plot_data and 'shap_values' in force_plot_data,
                f"Base value: {force_plot_data.get('base_value', 0):.3f}"
            )
            
            # èª¬æ˜ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = explainer.generate_explanation_report(X_single, "Test Product")
            self.add_test_result(
                "Explanation Report Generation",
                all(k in report for k in ['prediction', 'confidence', 'top_positive_factors']),
                f"Report keys: {list(report.keys())}"
            )
            
            # ãƒãƒƒãƒå‡¦ç†
            logger.info("Testing batch SHAP analysis...")
            X_batch = self.sample_data['X_val'].iloc[:10]
            batch_shap = explainer.calculate_shap_values(X_batch)
            
            self.add_test_result(
                "Batch SHAP Analysis",
                batch_shap.shape == X_batch.shape,
                f"Processed {len(X_batch)} samples"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "SHAP Analysis Module",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_optuna_optimization(self) -> bool:
        """Optunaæœ€é©åŒ–ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Optuna Hyperparameter Optimization")
        logger.info("="*50)
        
        try:
            if self.sample_data is None:
                raise ValueError("Test data not prepared")
            
            # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜é€Ÿãƒ†ã‚¹ãƒˆ
            X_train = self.sample_data['X_train'][:50]
            y_train = self.sample_data['y_train'][:50]
            X_val = self.sample_data['X_val'][:20]
            y_val = self.sample_data['y_val'][:20]
            
            # Random Forestæœ€é©åŒ–
            logger.info("Testing Random Forest optimization...")
            rf_optimizer = HyperparameterOptimizer(
                model_type='random_forest',
                n_trials=10  # ãƒ†ã‚¹ãƒˆç”¨ã«å°‘ãªã„è©¦è¡Œå›æ•°
            )
            
            rf_results = rf_optimizer.optimize(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "Random Forest Optimization",
                'best_params' in rf_results and 'best_cv_score' in rf_results,
                f"Best CV score: {rf_results.get('best_cv_score', 0):.3f}"
            )
            
            # XGBoostæœ€é©åŒ–
            logger.info("Testing XGBoost optimization...")
            xgb_optimizer = HyperparameterOptimizer(
                model_type='xgboost',
                n_trials=5  # ã‚ˆã‚Šå°‘ãªã„è©¦è¡Œå›æ•°
            )
            
            xgb_results = xgb_optimizer.optimize(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "XGBoost Optimization",
                'best_params' in xgb_results,
                f"Validation F1: {xgb_results.get('validation_metrics', {}).get('f1', 0):.3f}"
            )
            
            # LightGBMæœ€é©åŒ–
            logger.info("Testing LightGBM optimization...")
            lgb_optimizer = HyperparameterOptimizer(
                model_type='lightgbm',
                n_trials=5
            )
            
            lgb_results = lgb_optimizer.optimize(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "LightGBM Optimization",
                'best_params' in lgb_results,
                f"Validation accuracy: {lgb_results.get('validation_metrics', {}).get('accuracy', 0):.3f}"
            )
            
            # ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
            if rf_optimizer.best_model:
                importance_df = rf_optimizer.get_feature_importance(X_train.columns.tolist())
                self.add_test_result(
                    "Feature Importance from Optimized Model",
                    len(importance_df) > 0,
                    f"Top feature: {importance_df.iloc[0]['feature']}"
                )
            
            # ãƒ¢ãƒ‡ãƒ«ä¿å­˜ãƒ»èª­ã¿è¾¼ã¿ãƒ†ã‚¹ãƒˆ
            test_model_path = "test_optimized_model.pkl"
            rf_optimizer.save_model(test_model_path)
            
            new_optimizer = HyperparameterOptimizer()
            new_optimizer.load_model(test_model_path)
            
            self.add_test_result(
                "Model Save/Load",
                new_optimizer.best_model is not None,
                f"Model successfully saved and loaded"
            )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if os.path.exists(test_model_path):
                os.remove(test_model_path)
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Optuna Optimization",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_automl(self) -> bool:
        """AutoMLæ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing AutoML Functionality")
        logger.info("="*50)
        
        try:
            if self.sample_data is None:
                raise ValueError("Test data not prepared")
            
            # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§é«˜é€Ÿãƒ†ã‚¹ãƒˆ
            X_train = self.sample_data['X_train'][:30]
            y_train = self.sample_data['y_train'][:30]
            X_val = self.sample_data['X_val'][:10]
            y_val = self.sample_data['y_val'][:10]
            
            # AutoMLå®Ÿè¡Œ
            logger.info("Running AutoML...")
            automl = AutoML(n_trials=3)  # ãƒ†ã‚¹ãƒˆç”¨ã«éå¸¸ã«å°‘ãªã„è©¦è¡Œå›æ•°
            
            results = automl.fit(X_train, y_train, X_val, y_val)
            
            self.add_test_result(
                "AutoML Execution",
                'best_model_type' in results,
                f"Best model: {results.get('best_model_type', 'unknown')}"
            )
            
            # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
            predictions = automl.predict(X_val)
            pred_proba = automl.predict_proba(X_val)
            
            self.add_test_result(
                "AutoML Predictions",
                len(predictions) == len(X_val) and pred_proba.shape[0] == len(X_val),
                f"Generated {len(predictions)} predictions"
            )
            
            # ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒçµæœç¢ºèª
            if 'comparison' in results:
                comparison = pd.DataFrame(results['comparison'])
                self.add_test_result(
                    "Model Comparison",
                    len(comparison) > 0,
                    f"Compared {len(comparison)} models"
                )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "AutoML",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_streamlit_components(self) -> bool:
        """Streamlitã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆï¼ˆã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèªï¼‰"""
        logger.info("\n" + "="*50)
        logger.info("Testing Streamlit Components")
        logger.info("="*50)
        
        try:
            # Streamlit app ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            import streamlit_app
            
            self.add_test_result(
                "Streamlit App Import",
                True,
                "streamlit_app.py successfully imported"
            )
            
            # Dashboard ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆç¢ºèª
            import streamlit_dashboard
            
            self.add_test_result(
                "Dashboard Import",
                True,
                "streamlit_dashboard.py successfully imported"
            )
            
            # å¿…è¦ãªStreamlitã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ç¢ºèª
            import streamlit as st
            import plotly.graph_objects as go
            import plotly.express as px
            
            self.add_test_result(
                "Required Libraries",
                True,
                "All required visualization libraries available"
            )
            
            return True
            
        except ImportError as e:
            self.add_test_result(
                "Streamlit Components",
                False,
                f"Import error: {str(e)}"
            )
            return False
        except Exception as e:
            self.add_test_result(
                "Streamlit Components",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def test_integration(self) -> bool:
        """çµ±åˆãƒ†ã‚¹ãƒˆ"""
        logger.info("\n" + "="*50)
        logger.info("Testing Full Integration")
        logger.info("="*50)
        
        try:
            # ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
            pipeline = DataPipeline()
            engineer = FeatureEngineer()
            
            # 1. æ–°è£½å“ãƒ‡ãƒ¼ã‚¿
            new_product = {
                'name': 'Revolutionary Anti-Aging Serum',
                'keywords': ['retinol', 'peptide', 'vitamin C'],
                'price': 12000,
                'brand_strength': 0.8,
                'ingredient_novelty': 0.9,
                'market_saturation': 0.3
            }
            
            # 2. ç‰¹å¾´é‡æŠ½å‡º
            logger.info("Extracting features...")
            features = pipeline.extract_features(new_product)
            enhanced_features = engineer.create_advanced_features(features)
            
            self.add_test_result(
                "Feature Pipeline Integration",
                enhanced_features.shape[1] > features.shape[1],
                f"Enhanced from {features.shape[1]} to {enhanced_features.shape[1]} features"
            )
            
            # 3. äºˆæ¸¬å®Ÿè¡Œ
            if self.model:
                logger.info("Making prediction...")
                prediction = self.model.predict_with_confidence(enhanced_features)
                
                self.add_test_result(
                    "Prediction Pipeline",
                    'hit_probability' in prediction.columns,
                    f"Hit probability: {prediction['hit_probability'].iloc[0]:.2%}"
                )
                
                # 4. SHAPèª¬æ˜
                logger.info("Generating explanation...")
                explainer = ModelExplainer(self.model.model)
                explanation = explainer.generate_explanation_report(
                    enhanced_features,
                    new_product['name']
                )
                
                self.add_test_result(
                    "Explanation Integration",
                    'top_positive_factors' in explanation,
                    f"Generated {len(explanation.get('top_positive_factors', []))} positive factors"
                )
            
            # 5. æœ€é©åŒ–ã¨ã®çµ±åˆ
            logger.info("Testing optimization integration...")
            small_X = self.sample_data['X_train'][:20]
            small_y = self.sample_data['y_train'][:20]
            
            optimizer = HyperparameterOptimizer(
                model_type='random_forest',
                n_trials=3
            )
            opt_results = optimizer.optimize(small_X, small_y)
            
            self.add_test_result(
                "Optimization Integration",
                optimizer.best_model is not None,
                "Optimized model created successfully"
            )
            
            return True
            
        except Exception as e:
            self.add_test_result(
                "Full Integration",
                False,
                f"Error: {str(e)}"
            )
            return False
    
    def save_test_results(self):
        """ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜"""
        os.makedirs("tests", exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = f"tests/phase3_results_{timestamp}.json"
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(self.test_results, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"\nTest results saved to: {filepath}")
        
        return filepath
    
    def print_summary(self):
        """ãƒ†ã‚¹ãƒˆçµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º"""
        total = len(self.test_results['tests'])
        passed = sum(1 for t in self.test_results['tests'] if t['success'])
        failed = total - passed
        
        logger.info("\n" + "="*50)
        logger.info("PHASE 3 TEST SUMMARY")
        logger.info("="*50)
        logger.info(f"Total Tests: {total}")
        logger.info(f"âœ… Passed: {passed}")
        logger.info(f"âŒ Failed: {failed}")
        logger.info(f"Success Rate: {(passed/total*100):.1f}%")
        
        if failed > 0:
            logger.info("\nFailed Tests:")
            for test in self.test_results['tests']:
                if not test['success']:
                    logger.info(f"  - {test['name']}: {test['details']}")
        
        # Phase 3ã®æ–°æ©Ÿèƒ½ã‚µãƒãƒªãƒ¼
        logger.info("\n" + "="*50)
        logger.info("PHASE 3 FEATURES SUMMARY")
        logger.info("="*50)
        logger.info("âœ… SHAP Analysis for Model Explainability")
        logger.info("âœ… Streamlit Web UI (Prediction Interface)")
        logger.info("âœ… Optuna Hyperparameter Optimization")
        logger.info("âœ… AutoML with Model Comparison")
        logger.info("âœ… Real-time Dashboard with Metrics")
        logger.info("âœ… Advanced Visualizations")
        logger.info("âœ… Model Performance Monitoring")
        logger.info("âœ… Export and Reporting Features")
        
        return passed == total


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    logger.info("="*60)
    logger.info(" AI HIT PREDICTION SYSTEM - PHASE 3 TEST ")
    logger.info("="*60)
    logger.info(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    tester = Phase3Tester()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™
    if not tester.prepare_test_data():
        logger.error("Failed to prepare test data. Exiting.")
        return False
    
    # å„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_functions = [
        tester.test_shap_analysis,
        tester.test_optuna_optimization,
        tester.test_automl,
        tester.test_streamlit_components,
        tester.test_integration
    ]
    
    all_passed = True
    for test_func in test_functions:
        try:
            result = test_func()
            if not result:
                all_passed = False
        except Exception as e:
            logger.error(f"Test failed with exception: {e}")
            all_passed = False
    
    # çµæœã‚µãƒãƒªãƒ¼
    all_tests_passed = tester.print_summary()
    
    # çµæœä¿å­˜
    tester.save_test_results()
    
    # æœ€çµ‚çµæœ
    logger.info("\n" + "="*60)
    if all_tests_passed and all_passed:
        logger.info("ğŸ‰ ALL PHASE 3 TESTS PASSED! System is ready for deployment.")
        logger.info("\nTo launch the web UI:")
        logger.info("  1. Basic UI: streamlit run streamlit_app.py")
        logger.info("  2. Dashboard: streamlit run streamlit_dashboard.py")
    else:
        logger.info("âš ï¸  Some tests failed. Please review the results.")
    logger.info("="*60)
    
    return all_tests_passed


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    success = main()
    sys.exit(0 if success else 1)