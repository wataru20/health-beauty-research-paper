# AIヒット予測システム 実装計画書
## コード実装のための段階的アプローチ

---

## 🎯 実装方針

### 基本原則
1. **スモールスタート**: 最小限の機能から始めて段階的に拡張
2. **動作確認優先**: 各段階で動くものを作り、検証しながら進める
3. **Python中心**: 全ての実装をPythonで統一（データ収集、AI、Web UI）
4. **クラウド最小化**: 初期はローカル環境で開発、後からクラウド移行

---

## 📁 プロジェクト構造

```
ai-hit-prediction/
├── data/                    # データ保存用
│   ├── raw/                # 生データ
│   ├── processed/          # 加工済みデータ
│   └── models/             # 学習済みモデル
├── src/                    # ソースコード
│   ├── data_collection/    # データ収集
│   ├── preprocessing/      # データ前処理
│   ├── models/            # AIモデル
│   ├── analysis/          # 分析・可視化
│   └── api/               # API・Web UI
├── notebooks/              # Jupyter Notebook（実験用）
├── config/                 # 設定ファイル
├── tests/                  # テストコード
├── requirements.txt        # 必要なパッケージ
└── README.md              # プロジェクト説明
```

---

## 🚀 Phase 1: 基礎構築（1-2週間）

### 目標
最小限の動くプロトタイプを作成し、システムの基本動作を確認

### 実装タスク

#### 1.1 環境構築
```python
# requirements.txt の作成
pandas==2.1.0
numpy==1.25.0
scikit-learn==1.3.0
requests==2.31.0
python-dotenv==1.0.0
jupyter==1.0.0
```

```bash
# 実行コマンド
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 1.2 データ収集スクリプト（最小版）
```python
# src/data_collection/academic_collector.py

import requests
import json
from datetime import datetime
import time

class AcademicPaperCollector:
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/v1/paper/search"
        
    def search_papers(self, keyword, limit=10):
        """論文を検索して基本情報を取得"""
        params = {
            'query': keyword,
            'limit': limit
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"Error: {response.status_code}")
                return None
        except Exception as e:
            print(f"Exception: {e}")
            return None
    
    def save_to_file(self, data, keyword):
        """データをJSONファイルに保存"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/raw/papers_{keyword}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return filename

# 使用例
if __name__ == "__main__":
    collector = AcademicPaperCollector()
    
    # キーワードリスト
    keywords = ["vitamin C skincare", "retinol", "hyaluronic acid"]
    
    for keyword in keywords:
        print(f"Searching for: {keyword}")
        papers = collector.search_papers(keyword)
        if papers:
            filename = collector.save_to_file(papers, keyword.replace(" ", "_"))
            print(f"Saved to: {filename}")
        time.sleep(1)  # API制限対策
```

#### 1.3 簡易AIモデル実装
```python
# src/models/basic_model.py

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib

class HitPredictionModel:
    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        self.is_trained = False
    
    def prepare_features(self, data):
        """特徴量を準備（仮のデータ生成）"""
        # 実際のデータが揃うまでの仮実装
        features = pd.DataFrame({
            'scientific_trend_score': np.random.rand(len(data)),
            'social_mentions': np.random.randint(0, 1000, len(data)),
            'influencer_count': np.random.randint(0, 50, len(data)),
            'price_range': np.random.choice([1, 2, 3], len(data)),
            'competitor_count': np.random.randint(1, 20, len(data))
        })
        return features
    
    def train(self, X, y):
        """モデルを学習"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        self.model.fit(X_train, y_train)
        self.is_trained = True
        
        # 精度を確認
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        print(f"Training accuracy: {train_score:.2%}")
        print(f"Testing accuracy: {test_score:.2%}")
        
        return test_score
    
    def predict(self, X):
        """予測を実行"""
        if not self.is_trained:
            raise Exception("Model not trained yet")
        
        probabilities = self.model.predict_proba(X)
        return probabilities[:, 1]  # ヒットする確率
    
    def save_model(self, filepath):
        """モデルを保存"""
        joblib.dump(self.model, filepath)
        print(f"Model saved to: {filepath}")
    
    def load_model(self, filepath):
        """モデルを読み込み"""
        self.model = joblib.load(filepath)
        self.is_trained = True
        print(f"Model loaded from: {filepath}")

# テスト用のダミーデータ生成
if __name__ == "__main__":
    # ダミーデータ作成
    n_samples = 100
    dummy_data = pd.DataFrame({'id': range(n_samples)})
    
    model = HitPredictionModel()
    X = model.prepare_features(dummy_data)
    y = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # 30%がヒット
    
    # モデル学習
    model.train(X, y)
    
    # 予測テスト
    test_data = pd.DataFrame({'id': [101, 102]})
    X_test = model.prepare_features(test_data)
    predictions = model.predict(X_test)
    
    print(f"\nPredictions: {predictions}")
    
    # モデル保存
    model.save_model("data/models/basic_model.pkl")
```

#### 1.4 動作確認用スクリプト
```python
# test_integration.py

from src.data_collection.academic_collector import AcademicPaperCollector
from src.models.basic_model import HitPredictionModel
import pandas as pd

def test_full_pipeline():
    """全体の流れをテスト"""
    
    print("=== Phase 1: Data Collection ===")
    collector = AcademicPaperCollector()
    papers = collector.search_papers("vitamin C", limit=5)
    print(f"Collected {len(papers) if papers else 0} papers")
    
    print("\n=== Phase 2: Model Training ===")
    model = HitPredictionModel()
    # ダミーデータで学習
    dummy_data = pd.DataFrame({'id': range(50)})
    X = model.prepare_features(dummy_data)
    y = [1,0,1,0,1] * 10  # 仮のラベル
    accuracy = model.train(X, y)
    
    print("\n=== Phase 3: Prediction ===")
    new_product = pd.DataFrame({'id': [999]})
    X_new = model.prepare_features(new_product)
    hit_probability = model.predict(X_new)[0]
    print(f"Hit Probability: {hit_probability:.2%}")
    
    print("\n✅ All tests passed!")

if __name__ == "__main__":
    test_full_pipeline()
```

---

## 🚀 Phase 2: データ収集強化（2-3週間）

### 実装タスク

#### 2.1 ソーシャルデータ収集（NewsAPI利用）
```python
# src/data_collection/news_collector.py

import os
from newsapi import NewsApiClient
from datetime import datetime, timedelta
import json

class NewsCollector:
    def __init__(self):
        # .envファイルからAPIキーを読み込み
        self.api_key = os.getenv('NEWS_API_KEY', 'YOUR_API_KEY_HERE')
        self.newsapi = NewsApiClient(api_key=self.api_key)
    
    def search_news(self, keywords, days_back=7):
        """ニュース記事を検索"""
        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        articles = []
        for keyword in keywords:
            try:
                response = self.newsapi.get_everything(
                    q=keyword,
                    from_param=from_date,
                    language='en',
                    sort_by='popularity',
                    page_size=100
                )
                articles.extend(response['articles'])
            except Exception as e:
                print(f"Error for {keyword}: {e}")
        
        return articles
    
    def extract_trends(self, articles):
        """記事からトレンド情報を抽出"""
        trend_data = []
        for article in articles:
            trend_data.append({
                'title': article.get('title'),
                'source': article.get('source', {}).get('name'),
                'published_at': article.get('publishedAt'),
                'description': article.get('description'),
                'url': article.get('url')
            })
        return trend_data
```

#### 2.2 データ統合パイプライン
```python
# src/preprocessing/data_pipeline.py

import pandas as pd
from datetime import datetime
import json
import os

class DataPipeline:
    def __init__(self):
        self.raw_data_path = "data/raw/"
        self.processed_data_path = "data/processed/"
        
    def load_academic_data(self):
        """学術データを読み込み"""
        academic_files = [f for f in os.listdir(self.raw_data_path) 
                         if f.startswith('papers_')]
        
        all_papers = []
        for file in academic_files:
            with open(os.path.join(self.raw_data_path, file), 'r') as f:
                papers = json.load(f)
                all_papers.extend(papers)
        
        return pd.DataFrame(all_papers)
    
    def load_news_data(self):
        """ニュースデータを読み込み"""
        news_files = [f for f in os.listdir(self.raw_data_path) 
                     if f.startswith('news_')]
        
        all_news = []
        for file in news_files:
            with open(os.path.join(self.raw_data_path, file), 'r') as f:
                news = json.load(f)
                all_news.extend(news)
        
        return pd.DataFrame(all_news)
    
    def calculate_features(self, product_name):
        """製品の特徴量を計算"""
        features = {
            'scientific_trend_score': self.calculate_scientific_score(product_name),
            'social_buzz_score': self.calculate_social_score(product_name),
            'news_mentions': self.count_news_mentions(product_name),
            'timestamp': datetime.now().isoformat()
        }
        return features
    
    def calculate_scientific_score(self, product_name):
        """科学的トレンドスコアを計算"""
        papers = self.load_academic_data()
        # 簡易実装：タイトルに含まれる論文数をカウント
        if not papers.empty:
            count = papers['title'].str.contains(product_name, case=False).sum()
            return min(count / 10, 1.0)  # 正規化
        return 0.0
    
    def calculate_social_score(self, product_name):
        """ソーシャルバズスコアを計算"""
        # Phase 3で実装
        return 0.5  # 仮の値
    
    def count_news_mentions(self, product_name):
        """ニュース言及数をカウント"""
        news = self.load_news_data()
        if not news.empty:
            return news['title'].str.contains(product_name, case=False).sum()
        return 0
```

---

## 🚀 Phase 3: SHAP実装とUI構築（3-4週間）

### 実装タスク

#### 3.1 SHAP分析の実装
```python
# src/analysis/explainer.py

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class ModelExplainer:
    def __init__(self, model, feature_names):
        self.model = model
        self.feature_names = feature_names
        self.explainer = None
    
    def initialize_shap(self, background_data):
        """SHAP説明器を初期化"""
        self.explainer = shap.TreeExplainer(
            self.model, 
            background_data
        )
    
    def explain_prediction(self, X):
        """個別予測を説明"""
        if self.explainer is None:
            raise Exception("Explainer not initialized")
        
        shap_values = self.explainer.shap_values(X)
        
        # 二値分類の場合、正のクラスのSHAP値を使用
        if len(shap_values) == 2:
            shap_values = shap_values[1]
        
        return shap_values
    
    def create_force_plot(self, X_single, save_path=None):
        """フォースプロットを作成"""
        shap_values = self.explain_prediction(X_single)
        
        # HTMLとして保存
        plot = shap.force_plot(
            self.explainer.expected_value[1],
            shap_values[0],
            X_single.iloc[0],
            feature_names=self.feature_names,
            matplotlib=False
        )
        
        if save_path:
            shap.save_html(save_path, plot)
        
        return plot
    
    def create_summary_plot(self, X, save_path=None):
        """サマリープロットを作成"""
        shap_values = self.explain_prediction(X)
        
        plt.figure(figsize=(10, 6))
        shap.summary_plot(
            shap_values, 
            X, 
            feature_names=self.feature_names,
            show=False
        )
        
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
        
        plt.show()
    
    def get_feature_importance(self, X):
        """特徴量の重要度を取得"""
        shap_values = self.explain_prediction(X)
        
        # 絶対値の平均を重要度とする
        importance = np.abs(shap_values).mean(axis=0)
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        return importance_df
```

#### 3.2 簡易Web UI（Streamlit）
```python
# src/app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from src.models.basic_model import HitPredictionModel
from src.analysis.explainer import ModelExplainer
import joblib

# ページ設定
st.set_page_config(
    page_title="AIヒット予測システム",
    page_icon="🎯",
    layout="wide"
)

# タイトル
st.title("🎯 AIヒット予測システム")
st.markdown("新商品のヒット確率をAIで予測します")

# サイドバー
st.sidebar.header("商品情報入力")

# 入力フォーム
product_name = st.sidebar.text_input("商品名")
main_ingredient = st.sidebar.selectbox(
    "主要成分",
    ["ビタミンC", "レチノール", "ヒアルロン酸", "ナイアシンアミド", "その他"]
)
price = st.sidebar.number_input("価格（円）", min_value=0, value=5000, step=100)
target_age = st.sidebar.selectbox(
    "ターゲット年齢層",
    ["10-20代", "20-30代", "30-40代", "40-50代", "50代以上"]
)

# 予測ボタン
if st.sidebar.button("予測実行", type="primary"):
    if product_name:
        # プログレスバー
        progress = st.progress(0)
        status = st.empty()
        
        # Step 1: データ準備
        status.text("データを準備中...")
        progress.progress(25)
        
        # 特徴量を作成（実際にはAPIからデータ取得）
        features = pd.DataFrame({
            'scientific_trend_score': [0.7],
            'social_mentions': [450],
            'influencer_count': [12],
            'price_range': [2],
            'competitor_count': [8]
        })
        
        # Step 2: モデル読み込み
        status.text("AIモデルを読み込み中...")
        progress.progress(50)
        
        model = joblib.load("data/models/basic_model.pkl")
        
        # Step 3: 予測実行
        status.text("予測を実行中...")
        progress.progress(75)
        
        prediction = model.predict_proba(features)[0, 1]
        
        # Step 4: 完了
        progress.progress(100)
        status.text("予測完了！")
        
        # 結果表示
        st.header("予測結果")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "ヒット確率",
                f"{prediction:.1%}",
                delta=f"+{(prediction-0.5)*100:.1f}%"
            )
        
        with col2:
            # 信号機表示
            if prediction >= 0.7:
                st.success("🟢 高確率でヒット")
            elif prediction >= 0.4:
                st.warning("🟡 要改善")
            else:
                st.error("🔴 リスク高")
        
        with col3:
            st.metric("市場競合度", "中", delta="3社")
        
        # グラフ表示
        st.header("要因分析")
        
        # ダミーデータでグラフ作成
        factors = pd.DataFrame({
            '要因': ['科学的トレンド', 'SNS話題性', '価格競争力', 'インフルエンサー', '競合状況'],
            '影響度': [0.25, 0.15, 0.10, -0.08, -0.05]
        })
        
        fig = px.bar(
            factors, 
            x='影響度', 
            y='要因',
            orientation='h',
            color='影響度',
            color_continuous_scale=['red', 'yellow', 'green'],
            title="予測への影響要因"
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # 推奨アクション
        st.header("📋 推奨アクション")
        
        recommendations = [
            "✅ YouTubeでのインフルエンサーマーケティングを強化",
            "✅ 発売2週間前からティザー広告を開始",
            "⚠️ Instagram でのUGC生成施策を追加検討",
            "💡 科学的根拠を前面に出したPR戦略を採用"
        ]
        
        for rec in recommendations:
            st.write(rec)
    else:
        st.error("商品名を入力してください")

# 実行方法の表示
if __name__ == "__main__":
    st.sidebar.info(
        """
        ### 実行方法
        ```bash
        streamlit run src/app.py
        ```
        """
    )
```

---

## 🚀 Phase 4: 本格実装（4-6週間）

### 実装タスク

#### 4.1 Statusbrew/Social Insight連携
- OAuth認証の実装
- リアルタイムデータ取得
- センチメント分析

#### 4.2 ハイパーパラメータ最適化
```python
# src/models/optimizer.py

import optuna
from sklearn.model_selection import cross_val_score

class ModelOptimizer:
    def __init__(self, X, y):
        self.X = X
        self.y = y
    
    def objective(self, trial):
        """Optunaの目的関数"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 20),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
        }
        
        model = RandomForestClassifier(**params, random_state=42)
        score = cross_val_score(
            model, self.X, self.y, 
            cv=5, scoring='roc_auc'
        ).mean()
        
        return score
    
    def optimize(self, n_trials=100):
        """最適化を実行"""
        study = optuna.create_study(
            direction='maximize',
            study_name='hit_prediction_optimization'
        )
        
        study.optimize(self.objective, n_trials=n_trials)
        
        return study.best_params
```

#### 4.3 サクセスフォーミュラ実装
```python
# src/analysis/success_formula.py

class SuccessFormula:
    def __init__(self):
        self.kpis = self.define_kpis()
    
    def define_kpis(self):
        """KPIを定義"""
        return {
            'pre_launch': {
                'expert_reviews': {'target': 15, 'weight': 0.2},
                'teaser_engagement': {'target': 0.05, 'weight': 0.15}
            },
            'launch_week': {
                'ugc_count': {'target': 300, 'weight': 0.25},
                'search_volume_increase': {'target': 5.0, 'weight': 0.2}
            },
            'post_launch': {
                'review_count': {'target': 50, 'weight': 0.1},
                'return_rate': {'target': 0.2, 'weight': 0.1}
            }
        }
    
    def evaluate_product(self, product_metrics):
        """製品のKPI達成度を評価"""
        total_score = 0
        detailed_scores = {}
        
        for phase, kpis in self.kpis.items():
            phase_score = 0
            for kpi_name, kpi_info in kpis.items():
                if kpi_name in product_metrics.get(phase, {}):
                    actual = product_metrics[phase][kpi_name]
                    target = kpi_info['target']
                    achievement = min(actual / target, 1.0)
                    weighted_score = achievement * kpi_info['weight']
                    phase_score += weighted_score
                    
                    detailed_scores[f"{phase}_{kpi_name}"] = {
                        'actual': actual,
                        'target': target,
                        'achievement': achievement,
                        'weighted_score': weighted_score
                    }
            
            total_score += phase_score
        
        return {
            'total_score': total_score,
            'detailed_scores': detailed_scores
        }
    
    def generate_recommendations(self, evaluation):
        """改善推奨事項を生成"""
        recommendations = []
        
        for kpi, scores in evaluation['detailed_scores'].items():
            if scores['achievement'] < 0.7:
                recommendations.append({
                    'kpi': kpi,
                    'gap': scores['target'] - scores['actual'],
                    'priority': 'high' if scores['achievement'] < 0.3 else 'medium',
                    'action': self.get_action_for_kpi(kpi)
                })
        
        return sorted(recommendations, key=lambda x: x['priority'])
    
    def get_action_for_kpi(self, kpi):
        """KPIに応じたアクションを提案"""
        action_map = {
            'expert_reviews': '専門家インフルエンサーへのアプローチを強化',
            'ugc_count': 'ハッシュタグキャンペーンの実施',
            'search_volume_increase': 'SEO対策とリスティング広告の強化'
        }
        
        kpi_type = kpi.split('_', 1)[1]
        return action_map.get(kpi_type, 'マーケティング施策の見直し')
```

---

## 🚀 Phase 5: 運用準備（2-3週間）

### 実装タスク

#### 5.1 バッチ処理の自動化
```python
# src/batch/daily_update.py

import schedule
import time
from datetime import datetime

class DailyBatchProcessor:
    def __init__(self):
        self.tasks = []
    
    def collect_academic_data(self):
        """学術データを収集"""
        print(f"[{datetime.now()}] Collecting academic data...")
        # 実装
    
    def collect_social_data(self):
        """ソーシャルデータを収集"""
        print(f"[{datetime.now()}] Collecting social data...")
        # 実装
    
    def update_predictions(self):
        """予測モデルを更新"""
        print(f"[{datetime.now()}] Updating predictions...")
        # 実装
    
    def run_scheduler(self):
        """スケジューラーを実行"""
        # 毎日のタスクを設定
        schedule.every().day.at("02:00").do(self.collect_academic_data)
        schedule.every().day.at("06:00").do(self.collect_social_data)
        schedule.every().sunday.at("03:00").do(self.update_predictions)
        
        print("Batch scheduler started...")
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1分ごとにチェック
```

#### 5.2 エラー処理とログ
```python
# src/utils/logger.py

import logging
from datetime import datetime

class SystemLogger:
    def __init__(self):
        self.setup_logger()
    
    def setup_logger(self):
        """ロガーを設定"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'logs/system_{datetime.now():%Y%m%d}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_api_call(self, api_name, status, response_time):
        """API呼び出しをログ"""
        self.logger.info(
            f"API Call: {api_name} | Status: {status} | Time: {response_time}ms"
        )
    
    def log_prediction(self, product_name, probability, execution_time):
        """予測実行をログ"""
        self.logger.info(
            f"Prediction: {product_name} | Probability: {probability:.2%} | Time: {execution_time}ms"
        )
    
    def log_error(self, error_type, error_message, traceback=None):
        """エラーをログ"""
        self.logger.error(
            f"Error: {error_type} | Message: {error_message}"
        )
        if traceback:
            self.logger.error(f"Traceback: {traceback}")
```

---

## 📊 実装優先順位とタイムライン

### 即座に実装（Week 1）
1. プロジェクト構造の作成
2. 基本的なデータ収集（Semantic Scholar API）
3. 簡易モデルの実装

### 短期実装（Week 2-3）
4. NewsAPI連携
5. データパイプライン構築
6. Streamlit UI作成

### 中期実装（Week 4-6）
7. SHAP実装
8. Optuna最適化
9. より高度なデータ収集

### 長期実装（Week 7-8）
10. 本番環境構築
11. 自動化・バッチ処理
12. モニタリング・ログ

---

## 🛠️ 開発環境のセットアップ手順

```bash
# 1. プロジェクトフォルダ作成
mkdir ai-hit-prediction
cd ai-hit-prediction

# 2. Python仮想環境作成
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. 基本パッケージインストール
pip install pandas numpy scikit-learn requests python-dotenv jupyter

# 4. フォルダ構造作成
mkdir -p data/{raw,processed,models}
mkdir -p src/{data_collection,preprocessing,models,analysis,api}
mkdir notebooks config tests logs

# 5. 環境変数ファイル作成
echo "NEWS_API_KEY=your_key_here" > .env

# 6. Gitリポジトリ初期化
git init
echo "venv/" > .gitignore
echo ".env" >> .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignore
echo "logs/" >> .gitignore

# 7. 最初のコミット
git add .
git commit -m "Initial project setup"
```

---

## 🎯 次のステップ

1. **まず実装すべきファイル**
   - `requirements.txt`
   - `src/data_collection/academic_collector.py`
   - `src/models/basic_model.py`
   - `test_integration.py`

2. **動作確認**
   ```bash
   python test_integration.py
   ```

3. **段階的な機能追加**
   - Phase 1を完了してから次へ
   - 各フェーズで動作確認
   - 問題があれば修正してから進む

---

## 📝 メモ

- 各ファイルは独立して動作するように設計
- エラーが出ても他の部分に影響しないモジュール構成
- 徐々に機能を追加していく成長型アーキテクチャ
- 実データが揃うまではダミーデータで動作確認

---

*この計画書に従って、一つずつ実装を進めていきます。*
*各ステップで動作確認をしながら、着実に構築していきましょう。*