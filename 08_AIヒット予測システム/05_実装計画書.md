# AIãƒ’ãƒƒãƒˆäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ  å®Ÿè£…è¨ˆç”»æ›¸
## ã‚³ãƒ¼ãƒ‰å®Ÿè£…ã®ãŸã‚ã®æ®µéšçš„ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ

---

## ğŸ¯ å®Ÿè£…æ–¹é‡

### åŸºæœ¬åŸå‰‡
1. **ã‚¹ãƒ¢ãƒ¼ãƒ«ã‚¹ã‚¿ãƒ¼ãƒˆ**: æœ€å°é™ã®æ©Ÿèƒ½ã‹ã‚‰å§‹ã‚ã¦æ®µéšçš„ã«æ‹¡å¼µ
2. **å‹•ä½œç¢ºèªå„ªå…ˆ**: å„æ®µéšã§å‹•ãã‚‚ã®ã‚’ä½œã‚Šã€æ¤œè¨¼ã—ãªãŒã‚‰é€²ã‚ã‚‹
3. **Pythonä¸­å¿ƒ**: å…¨ã¦ã®å®Ÿè£…ã‚’Pythonã§çµ±ä¸€ï¼ˆãƒ‡ãƒ¼ã‚¿åé›†ã€AIã€Web UIï¼‰
4. **ã‚¯ãƒ©ã‚¦ãƒ‰æœ€å°åŒ–**: åˆæœŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§é–‹ç™ºã€å¾Œã‹ã‚‰ã‚¯ãƒ©ã‚¦ãƒ‰ç§»è¡Œ

---

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
ai-hit-prediction/
â”œâ”€â”€ data/                    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜ç”¨
â”‚   â”œâ”€â”€ raw/                # ç”Ÿãƒ‡ãƒ¼ã‚¿
â”‚   â”œâ”€â”€ processed/          # åŠ å·¥æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿
â”‚   â””â”€â”€ models/             # å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
â”œâ”€â”€ src/                    # ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰
â”‚   â”œâ”€â”€ data_collection/    # ãƒ‡ãƒ¼ã‚¿åé›†
â”‚   â”œâ”€â”€ preprocessing/      # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†
â”‚   â”œâ”€â”€ models/            # AIãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ analysis/          # åˆ†æãƒ»å¯è¦–åŒ–
â”‚   â””â”€â”€ api/               # APIãƒ»Web UI
â”œâ”€â”€ notebooks/              # Jupyter Notebookï¼ˆå®Ÿé¨“ç”¨ï¼‰
â”œâ”€â”€ config/                 # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«
â”œâ”€â”€ tests/                  # ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰
â”œâ”€â”€ requirements.txt        # å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
â””â”€â”€ README.md              # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆèª¬æ˜
```

---

## ğŸš€ Phase 1: åŸºç¤æ§‹ç¯‰ï¼ˆ1-2é€±é–“ï¼‰

### ç›®æ¨™
æœ€å°é™ã®å‹•ããƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’ä½œæˆã—ã€ã‚·ã‚¹ãƒ†ãƒ ã®åŸºæœ¬å‹•ä½œã‚’ç¢ºèª

### å®Ÿè£…ã‚¿ã‚¹ã‚¯

#### 1.1 ç’°å¢ƒæ§‹ç¯‰
```python
# requirements.txt ã®ä½œæˆ
pandas==2.1.0
numpy==1.25.0
scikit-learn==1.3.0
requests==2.31.0
python-dotenv==1.0.0
jupyter==1.0.0
```

```bash
# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 1.2 ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæœ€å°ç‰ˆï¼‰
```python
# src/data_collection/academic_collector.py

import requests
import json
from datetime import datetime
import time

class AcademicPaperCollector:
    def __init__(self):
        self.base_url = "https://api.semanticscholar.org/v1/paper/search"
        
    def search_papers(self, keyword, limit=10):
        """è«–æ–‡ã‚’æ¤œç´¢ã—ã¦åŸºæœ¬æƒ…å ±ã‚’å–å¾—"""
        params = {
            'query': keyword,
            'limit': limit
        }
        
        try:
            response = requests.get(self.base_url, params=params)
            if response.status_code == 200:
                return response.json()
            else:
                print(f"Error: {response.status_code}")
                return None
        except Exception as e:
            print(f"Exception: {e}")
            return None
    
    def save_to_file(self, data, keyword):
        """ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"data/raw/papers_{keyword}_{timestamp}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return filename

# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    collector = AcademicPaperCollector()
    
    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
    keywords = ["vitamin C skincare", "retinol", "hyaluronic acid"]
    
    for keyword in keywords:
        print(f"Searching for: {keyword}")
        papers = collector.search_papers(keyword)
        if papers:
            filename = collector.save_to_file(papers, keyword.replace(" ", "_"))
            print(f"Saved to: {filename}")
        time.sleep(1)  # APIåˆ¶é™å¯¾ç­–
```

#### 1.3 ç°¡æ˜“AIãƒ¢ãƒ‡ãƒ«å®Ÿè£…
```python
# src/models/basic_model.py

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
import joblib

class HitPredictionModel:
    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42
        )
        self.is_trained = False
    
    def prepare_features(self, data):
        """ç‰¹å¾´é‡ã‚’æº–å‚™ï¼ˆä»®ã®ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆï¼‰"""
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ãŒæƒã†ã¾ã§ã®ä»®å®Ÿè£…
        features = pd.DataFrame({
            'scientific_trend_score': np.random.rand(len(data)),
            'social_mentions': np.random.randint(0, 1000, len(data)),
            'influencer_count': np.random.randint(0, 50, len(data)),
            'price_range': np.random.choice([1, 2, 3], len(data)),
            'competitor_count': np.random.randint(1, 20, len(data))
        })
        return features
    
    def train(self, X, y):
        """ãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’"""
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        self.model.fit(X_train, y_train)
        self.is_trained = True
        
        # ç²¾åº¦ã‚’ç¢ºèª
        train_score = self.model.score(X_train, y_train)
        test_score = self.model.score(X_test, y_test)
        
        print(f"Training accuracy: {train_score:.2%}")
        print(f"Testing accuracy: {test_score:.2%}")
        
        return test_score
    
    def predict(self, X):
        """äºˆæ¸¬ã‚’å®Ÿè¡Œ"""
        if not self.is_trained:
            raise Exception("Model not trained yet")
        
        probabilities = self.model.predict_proba(X)
        return probabilities[:, 1]  # ãƒ’ãƒƒãƒˆã™ã‚‹ç¢ºç‡
    
    def save_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
        joblib.dump(self.model, filepath)
        print(f"Model saved to: {filepath}")
    
    def load_model(self, filepath):
        """ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        self.model = joblib.load(filepath)
        self.is_trained = True
        print(f"Model loaded from: {filepath}")

# ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
if __name__ == "__main__":
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ä½œæˆ
    n_samples = 100
    dummy_data = pd.DataFrame({'id': range(n_samples)})
    
    model = HitPredictionModel()
    X = model.prepare_features(dummy_data)
    y = np.random.choice([0, 1], n_samples, p=[0.7, 0.3])  # 30%ãŒãƒ’ãƒƒãƒˆ
    
    # ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
    model.train(X, y)
    
    # äºˆæ¸¬ãƒ†ã‚¹ãƒˆ
    test_data = pd.DataFrame({'id': [101, 102]})
    X_test = model.prepare_features(test_data)
    predictions = model.predict(X_test)
    
    print(f"\nPredictions: {predictions}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model.save_model("data/models/basic_model.pkl")
```

#### 1.4 å‹•ä½œç¢ºèªç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python
# test_integration.py

from src.data_collection.academic_collector import AcademicPaperCollector
from src.models.basic_model import HitPredictionModel
import pandas as pd

def test_full_pipeline():
    """å…¨ä½“ã®æµã‚Œã‚’ãƒ†ã‚¹ãƒˆ"""
    
    print("=== Phase 1: Data Collection ===")
    collector = AcademicPaperCollector()
    papers = collector.search_papers("vitamin C", limit=5)
    print(f"Collected {len(papers) if papers else 0} papers")
    
    print("\n=== Phase 2: Model Training ===")
    model = HitPredictionModel()
    # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’
    dummy_data = pd.DataFrame({'id': range(50)})
    X = model.prepare_features(dummy_data)
    y = [1,0,1,0,1] * 10  # ä»®ã®ãƒ©ãƒ™ãƒ«
    accuracy = model.train(X, y)
    
    print("\n=== Phase 3: Prediction ===")
    new_product = pd.DataFrame({'id': [999]})
    X_new = model.prepare_features(new_product)
    hit_probability = model.predict(X_new)[0]
    print(f"Hit Probability: {hit_probability:.2%}")
    
    print("\nâœ… All tests passed!")

if __name__ == "__main__":
    test_full_pipeline()
```

---

## ğŸš€ Phase 2: ãƒ‡ãƒ¼ã‚¿åé›†å¼·åŒ–ï¼ˆ2-3é€±é–“ï¼‰

### å®Ÿè£…ã‚¿ã‚¹ã‚¯

#### 2.1 ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆNewsAPIåˆ©ç”¨ï¼‰
```python
# src/data_collection/news_collector.py

import os
from newsapi import NewsApiClient
from datetime import datetime, timedelta
import json

class NewsCollector:
    def __init__(self):
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰APIã‚­ãƒ¼ã‚’èª­ã¿è¾¼ã¿
        self.api_key = os.getenv('NEWS_API_KEY', 'YOUR_API_KEY_HERE')
        self.newsapi = NewsApiClient(api_key=self.api_key)
    
    def search_news(self, keywords, days_back=7):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ¤œç´¢"""
        from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
        
        articles = []
        for keyword in keywords:
            try:
                response = self.newsapi.get_everything(
                    q=keyword,
                    from_param=from_date,
                    language='en',
                    sort_by='popularity',
                    page_size=100
                )
                articles.extend(response['articles'])
            except Exception as e:
                print(f"Error for {keyword}: {e}")
        
        return articles
    
    def extract_trends(self, articles):
        """è¨˜äº‹ã‹ã‚‰ãƒˆãƒ¬ãƒ³ãƒ‰æƒ…å ±ã‚’æŠ½å‡º"""
        trend_data = []
        for article in articles:
            trend_data.append({
                'title': article.get('title'),
                'source': article.get('source', {}).get('name'),
                'published_at': article.get('publishedAt'),
                'description': article.get('description'),
                'url': article.get('url')
            })
        return trend_data
```

#### 2.2 ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
```python
# src/preprocessing/data_pipeline.py

import pandas as pd
from datetime import datetime
import json
import os

class DataPipeline:
    def __init__(self):
        self.raw_data_path = "data/raw/"
        self.processed_data_path = "data/processed/"
        
    def load_academic_data(self):
        """å­¦è¡“ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        academic_files = [f for f in os.listdir(self.raw_data_path) 
                         if f.startswith('papers_')]
        
        all_papers = []
        for file in academic_files:
            with open(os.path.join(self.raw_data_path, file), 'r') as f:
                papers = json.load(f)
                all_papers.extend(papers)
        
        return pd.DataFrame(all_papers)
    
    def load_news_data(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        news_files = [f for f in os.listdir(self.raw_data_path) 
                     if f.startswith('news_')]
        
        all_news = []
        for file in news_files:
            with open(os.path.join(self.raw_data_path, file), 'r') as f:
                news = json.load(f)
                all_news.extend(news)
        
        return pd.DataFrame(all_news)
    
    def calculate_features(self, product_name):
        """è£½å“ã®ç‰¹å¾´é‡ã‚’è¨ˆç®—"""
        features = {
            'scientific_trend_score': self.calculate_scientific_score(product_name),
            'social_buzz_score': self.calculate_social_score(product_name),
            'news_mentions': self.count_news_mentions(product_name),
            'timestamp': datetime.now().isoformat()
        }
        return features
    
    def calculate_scientific_score(self, product_name):
        """ç§‘å­¦çš„ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        papers = self.load_academic_data()
        # ç°¡æ˜“å®Ÿè£…ï¼šã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹è«–æ–‡æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        if not papers.empty:
            count = papers['title'].str.contains(product_name, case=False).sum()
            return min(count / 10, 1.0)  # æ­£è¦åŒ–
        return 0.0
    
    def calculate_social_score(self, product_name):
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒã‚ºã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        # Phase 3ã§å®Ÿè£…
        return 0.5  # ä»®ã®å€¤
    
    def count_news_mentions(self, product_name):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨€åŠæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        news = self.load_news_data()
        if not news.empty:
            return news['title'].str.contains(product_name, case=False).sum()
        return 0
```

---

## ğŸš€ Phase 3: SHAPå®Ÿè£…ã¨UIæ§‹ç¯‰ï¼ˆ3-4é€±é–“ï¼‰

### å®Ÿè£…ã‚¿ã‚¹ã‚¯

#### 3.1 SHAPåˆ†æã®å®Ÿè£…
```python
# src/analysis/explainer.py

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class ModelExplainer:
    def __init__(self, model, feature_names):
        self.model = model
        self.feature_names = feature_names
        self.explainer = None
    
    def initialize_shap(self, background_data):
        """SHAPèª¬æ˜å™¨ã‚’åˆæœŸåŒ–"""
        self.explainer = shap.TreeExplainer(
            self.model, 
            background_data
        )
    
    def explain_prediction(self, X):
        """å€‹åˆ¥äºˆæ¸¬ã‚’èª¬æ˜"""
        if self.explainer is None:
            raise Exception("Explainer not initialized")
        
        shap_values = self.explainer.shap_values(X)
        
        # äºŒå€¤åˆ†é¡ã®å ´åˆã€æ­£ã®ã‚¯ãƒ©ã‚¹ã®SHAPå€¤ã‚’ä½¿ç”¨
        if len(shap_values) == 2:
            shap_values = shap_values[1]
        
        return shap_values
    
    def create_force_plot(self, X_single, save_path=None):
        """ãƒ•ã‚©ãƒ¼ã‚¹ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
        shap_values = self.explain_prediction(X_single)
        
        # HTMLã¨ã—ã¦ä¿å­˜
        plot = shap.force_plot(
            self.explainer.expected_value[1],
            shap_values[0],
            X_single.iloc[0],
            feature_names=self.feature_names,
            matplotlib=False
        )
        
        if save_path:
            shap.save_html(save_path, plot)
        
        return plot
    
    def create_summary_plot(self, X, save_path=None):
        """ã‚µãƒãƒªãƒ¼ãƒ—ãƒ­ãƒƒãƒˆã‚’ä½œæˆ"""
        shap_values = self.explain_prediction(X)
        
        plt.figure(figsize=(10, 6))
        shap.summary_plot(
            shap_values, 
            X, 
            feature_names=self.feature_names,
            show=False
        )
        
        if save_path:
            plt.savefig(save_path, dpi=100, bbox_inches='tight')
        
        plt.show()
    
    def get_feature_importance(self, X):
        """ç‰¹å¾´é‡ã®é‡è¦åº¦ã‚’å–å¾—"""
        shap_values = self.explain_prediction(X)
        
        # çµ¶å¯¾å€¤ã®å¹³å‡ã‚’é‡è¦åº¦ã¨ã™ã‚‹
        importance = np.abs(shap_values).mean(axis=0)
        
        importance_df = pd.DataFrame({
            'feature': self.feature_names,
            'importance': importance
        }).sort_values('importance', ascending=False)
        
        return importance_df
```

#### 3.2 ç°¡æ˜“Web UIï¼ˆStreamlitï¼‰
```python
# src/app.py

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from src.models.basic_model import HitPredictionModel
from src.analysis.explainer import ModelExplainer
import joblib

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="AIãƒ’ãƒƒãƒˆäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ¯",
    layout="wide"
)

# ã‚¿ã‚¤ãƒˆãƒ«
st.title("ğŸ¯ AIãƒ’ãƒƒãƒˆäºˆæ¸¬ã‚·ã‚¹ãƒ†ãƒ ")
st.markdown("æ–°å•†å“ã®ãƒ’ãƒƒãƒˆç¢ºç‡ã‚’AIã§äºˆæ¸¬ã—ã¾ã™")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
st.sidebar.header("å•†å“æƒ…å ±å…¥åŠ›")

# å…¥åŠ›ãƒ•ã‚©ãƒ¼ãƒ 
product_name = st.sidebar.text_input("å•†å“å")
main_ingredient = st.sidebar.selectbox(
    "ä¸»è¦æˆåˆ†",
    ["ãƒ“ã‚¿ãƒŸãƒ³C", "ãƒ¬ãƒãƒãƒ¼ãƒ«", "ãƒ’ã‚¢ãƒ«ãƒ­ãƒ³é…¸", "ãƒŠã‚¤ã‚¢ã‚·ãƒ³ã‚¢ãƒŸãƒ‰", "ãã®ä»–"]
)
price = st.sidebar.number_input("ä¾¡æ ¼ï¼ˆå††ï¼‰", min_value=0, value=5000, step=100)
target_age = st.sidebar.selectbox(
    "ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¹´é½¢å±¤",
    ["10-20ä»£", "20-30ä»£", "30-40ä»£", "40-50ä»£", "50ä»£ä»¥ä¸Š"]
)

# äºˆæ¸¬ãƒœã‚¿ãƒ³
if st.sidebar.button("äºˆæ¸¬å®Ÿè¡Œ", type="primary"):
    if product_name:
        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
        progress = st.progress(0)
        status = st.empty()
        
        # Step 1: ãƒ‡ãƒ¼ã‚¿æº–å‚™
        status.text("ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ä¸­...")
        progress.progress(25)
        
        # ç‰¹å¾´é‡ã‚’ä½œæˆï¼ˆå®Ÿéš›ã«ã¯APIã‹ã‚‰ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼‰
        features = pd.DataFrame({
            'scientific_trend_score': [0.7],
            'social_mentions': [450],
            'influencer_count': [12],
            'price_range': [2],
            'competitor_count': [8]
        })
        
        # Step 2: ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        status.text("AIãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
        progress.progress(50)
        
        model = joblib.load("data/models/basic_model.pkl")
        
        # Step 3: äºˆæ¸¬å®Ÿè¡Œ
        status.text("äºˆæ¸¬ã‚’å®Ÿè¡Œä¸­...")
        progress.progress(75)
        
        prediction = model.predict_proba(features)[0, 1]
        
        # Step 4: å®Œäº†
        progress.progress(100)
        status.text("äºˆæ¸¬å®Œäº†ï¼")
        
        # çµæœè¡¨ç¤º
        st.header("äºˆæ¸¬çµæœ")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "ãƒ’ãƒƒãƒˆç¢ºç‡",
                f"{prediction:.1%}",
                delta=f"+{(prediction-0.5)*100:.1f}%"
            )
        
        with col2:
            # ä¿¡å·æ©Ÿè¡¨ç¤º
            if prediction >= 0.7:
                st.success("ğŸŸ¢ é«˜ç¢ºç‡ã§ãƒ’ãƒƒãƒˆ")
            elif prediction >= 0.4:
                st.warning("ğŸŸ¡ è¦æ”¹å–„")
            else:
                st.error("ğŸ”´ ãƒªã‚¹ã‚¯é«˜")
        
        with col3:
            st.metric("å¸‚å ´ç«¶åˆåº¦", "ä¸­", delta="3ç¤¾")
        
        # ã‚°ãƒ©ãƒ•è¡¨ç¤º
        st.header("è¦å› åˆ†æ")
        
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§ã‚°ãƒ©ãƒ•ä½œæˆ
        factors = pd.DataFrame({
            'è¦å› ': ['ç§‘å­¦çš„ãƒˆãƒ¬ãƒ³ãƒ‰', 'SNSè©±é¡Œæ€§', 'ä¾¡æ ¼ç«¶äº‰åŠ›', 'ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼', 'ç«¶åˆçŠ¶æ³'],
            'å½±éŸ¿åº¦': [0.25, 0.15, 0.10, -0.08, -0.05]
        })
        
        fig = px.bar(
            factors, 
            x='å½±éŸ¿åº¦', 
            y='è¦å› ',
            orientation='h',
            color='å½±éŸ¿åº¦',
            color_continuous_scale=['red', 'yellow', 'green'],
            title="äºˆæ¸¬ã¸ã®å½±éŸ¿è¦å› "
        )
        
        st.plotly_chart(fig, use_container_width=True)
        
        # æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³
        st.header("ğŸ“‹ æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³")
        
        recommendations = [
            "âœ… YouTubeã§ã®ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã‚’å¼·åŒ–",
            "âœ… ç™ºå£²2é€±é–“å‰ã‹ã‚‰ãƒ†ã‚£ã‚¶ãƒ¼åºƒå‘Šã‚’é–‹å§‹",
            "âš ï¸ Instagram ã§ã®UGCç”Ÿæˆæ–½ç­–ã‚’è¿½åŠ æ¤œè¨",
            "ğŸ’¡ ç§‘å­¦çš„æ ¹æ‹ ã‚’å‰é¢ã«å‡ºã—ãŸPRæˆ¦ç•¥ã‚’æ¡ç”¨"
        ]
        
        for rec in recommendations:
            st.write(rec)
    else:
        st.error("å•†å“åã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")

# å®Ÿè¡Œæ–¹æ³•ã®è¡¨ç¤º
if __name__ == "__main__":
    st.sidebar.info(
        """
        ### å®Ÿè¡Œæ–¹æ³•
        ```bash
        streamlit run src/app.py
        ```
        """
    )
```

---

## ğŸš€ Phase 4: æœ¬æ ¼å®Ÿè£…ï¼ˆ4-6é€±é–“ï¼‰

### å®Ÿè£…ã‚¿ã‚¹ã‚¯

#### 4.1 Statusbrew/Social Insighté€£æº
- OAuthèªè¨¼ã®å®Ÿè£…
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿å–å¾—
- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ

#### 4.2 ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–
```python
# src/models/optimizer.py

import optuna
from sklearn.model_selection import cross_val_score

class ModelOptimizer:
    def __init__(self, X, y):
        self.X = X
        self.y = y
    
    def objective(self, trial):
        """Optunaã®ç›®çš„é–¢æ•°"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 50, 300),
            'max_depth': trial.suggest_int('max_depth', 3, 20),
            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),
            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10)
        }
        
        model = RandomForestClassifier(**params, random_state=42)
        score = cross_val_score(
            model, self.X, self.y, 
            cv=5, scoring='roc_auc'
        ).mean()
        
        return score
    
    def optimize(self, n_trials=100):
        """æœ€é©åŒ–ã‚’å®Ÿè¡Œ"""
        study = optuna.create_study(
            direction='maximize',
            study_name='hit_prediction_optimization'
        )
        
        study.optimize(self.objective, n_trials=n_trials)
        
        return study.best_params
```

#### 4.3 ã‚µã‚¯ã‚»ã‚¹ãƒ•ã‚©ãƒ¼ãƒŸãƒ¥ãƒ©å®Ÿè£…
```python
# src/analysis/success_formula.py

class SuccessFormula:
    def __init__(self):
        self.kpis = self.define_kpis()
    
    def define_kpis(self):
        """KPIã‚’å®šç¾©"""
        return {
            'pre_launch': {
                'expert_reviews': {'target': 15, 'weight': 0.2},
                'teaser_engagement': {'target': 0.05, 'weight': 0.15}
            },
            'launch_week': {
                'ugc_count': {'target': 300, 'weight': 0.25},
                'search_volume_increase': {'target': 5.0, 'weight': 0.2}
            },
            'post_launch': {
                'review_count': {'target': 50, 'weight': 0.1},
                'return_rate': {'target': 0.2, 'weight': 0.1}
            }
        }
    
    def evaluate_product(self, product_metrics):
        """è£½å“ã®KPIé”æˆåº¦ã‚’è©•ä¾¡"""
        total_score = 0
        detailed_scores = {}
        
        for phase, kpis in self.kpis.items():
            phase_score = 0
            for kpi_name, kpi_info in kpis.items():
                if kpi_name in product_metrics.get(phase, {}):
                    actual = product_metrics[phase][kpi_name]
                    target = kpi_info['target']
                    achievement = min(actual / target, 1.0)
                    weighted_score = achievement * kpi_info['weight']
                    phase_score += weighted_score
                    
                    detailed_scores[f"{phase}_{kpi_name}"] = {
                        'actual': actual,
                        'target': target,
                        'achievement': achievement,
                        'weighted_score': weighted_score
                    }
            
            total_score += phase_score
        
        return {
            'total_score': total_score,
            'detailed_scores': detailed_scores
        }
    
    def generate_recommendations(self, evaluation):
        """æ”¹å–„æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        for kpi, scores in evaluation['detailed_scores'].items():
            if scores['achievement'] < 0.7:
                recommendations.append({
                    'kpi': kpi,
                    'gap': scores['target'] - scores['actual'],
                    'priority': 'high' if scores['achievement'] < 0.3 else 'medium',
                    'action': self.get_action_for_kpi(kpi)
                })
        
        return sorted(recommendations, key=lambda x: x['priority'])
    
    def get_action_for_kpi(self, kpi):
        """KPIã«å¿œã˜ãŸã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’ææ¡ˆ"""
        action_map = {
            'expert_reviews': 'å°‚é–€å®¶ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã¸ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å¼·åŒ–',
            'ugc_count': 'ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ã®å®Ÿæ–½',
            'search_volume_increase': 'SEOå¯¾ç­–ã¨ãƒªã‚¹ãƒ†ã‚£ãƒ³ã‚°åºƒå‘Šã®å¼·åŒ–'
        }
        
        kpi_type = kpi.split('_', 1)[1]
        return action_map.get(kpi_type, 'ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°æ–½ç­–ã®è¦‹ç›´ã—')
```

---

## ğŸš€ Phase 5: é‹ç”¨æº–å‚™ï¼ˆ2-3é€±é–“ï¼‰

### å®Ÿè£…ã‚¿ã‚¹ã‚¯

#### 5.1 ãƒãƒƒãƒå‡¦ç†ã®è‡ªå‹•åŒ–
```python
# src/batch/daily_update.py

import schedule
import time
from datetime import datetime

class DailyBatchProcessor:
    def __init__(self):
        self.tasks = []
    
    def collect_academic_data(self):
        """å­¦è¡“ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"[{datetime.now()}] Collecting academic data...")
        # å®Ÿè£…
    
    def collect_social_data(self):
        """ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""
        print(f"[{datetime.now()}] Collecting social data...")
        # å®Ÿè£…
    
    def update_predictions(self):
        """äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ›´æ–°"""
        print(f"[{datetime.now()}] Updating predictions...")
        # å®Ÿè£…
    
    def run_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ"""
        # æ¯æ—¥ã®ã‚¿ã‚¹ã‚¯ã‚’è¨­å®š
        schedule.every().day.at("02:00").do(self.collect_academic_data)
        schedule.every().day.at("06:00").do(self.collect_social_data)
        schedule.every().sunday.at("03:00").do(self.update_predictions)
        
        print("Batch scheduler started...")
        while True:
            schedule.run_pending()
            time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
```

#### 5.2 ã‚¨ãƒ©ãƒ¼å‡¦ç†ã¨ãƒ­ã‚°
```python
# src/utils/logger.py

import logging
from datetime import datetime

class SystemLogger:
    def __init__(self):
        self.setup_logger()
    
    def setup_logger(self):
        """ãƒ­ã‚¬ãƒ¼ã‚’è¨­å®š"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f'logs/system_{datetime.now():%Y%m%d}.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    def log_api_call(self, api_name, status, response_time):
        """APIå‘¼ã³å‡ºã—ã‚’ãƒ­ã‚°"""
        self.logger.info(
            f"API Call: {api_name} | Status: {status} | Time: {response_time}ms"
        )
    
    def log_prediction(self, product_name, probability, execution_time):
        """äºˆæ¸¬å®Ÿè¡Œã‚’ãƒ­ã‚°"""
        self.logger.info(
            f"Prediction: {product_name} | Probability: {probability:.2%} | Time: {execution_time}ms"
        )
    
    def log_error(self, error_type, error_message, traceback=None):
        """ã‚¨ãƒ©ãƒ¼ã‚’ãƒ­ã‚°"""
        self.logger.error(
            f"Error: {error_type} | Message: {error_message}"
        )
        if traceback:
            self.logger.error(f"Traceback: {traceback}")
```

---

## ğŸ“Š å®Ÿè£…å„ªå…ˆé †ä½ã¨ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³

### å³åº§ã«å®Ÿè£…ï¼ˆWeek 1ï¼‰
1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®ä½œæˆ
2. åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆSemantic Scholar APIï¼‰
3. ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

### çŸ­æœŸå®Ÿè£…ï¼ˆWeek 2-3ï¼‰
4. NewsAPIé€£æº
5. ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
6. Streamlit UIä½œæˆ

### ä¸­æœŸå®Ÿè£…ï¼ˆWeek 4-6ï¼‰
7. SHAPå®Ÿè£…
8. Optunaæœ€é©åŒ–
9. ã‚ˆã‚Šé«˜åº¦ãªãƒ‡ãƒ¼ã‚¿åé›†

### é•·æœŸå®Ÿè£…ï¼ˆWeek 7-8ï¼‰
10. æœ¬ç•ªç’°å¢ƒæ§‹ç¯‰
11. è‡ªå‹•åŒ–ãƒ»ãƒãƒƒãƒå‡¦ç†
12. ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ãƒ»ãƒ­ã‚°

---

## ğŸ› ï¸ é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †

```bash
# 1. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚©ãƒ«ãƒ€ä½œæˆ
mkdir ai-hit-prediction
cd ai-hit-prediction

# 2. Pythonä»®æƒ³ç’°å¢ƒä½œæˆ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. åŸºæœ¬ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install pandas numpy scikit-learn requests python-dotenv jupyter

# 4. ãƒ•ã‚©ãƒ«ãƒ€æ§‹é€ ä½œæˆ
mkdir -p data/{raw,processed,models}
mkdir -p src/{data_collection,preprocessing,models,analysis,api}
mkdir notebooks config tests logs

# 5. ç’°å¢ƒå¤‰æ•°ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
echo "NEWS_API_KEY=your_key_here" > .env

# 6. Gitãƒªãƒã‚¸ãƒˆãƒªåˆæœŸåŒ–
git init
echo "venv/" > .gitignore
echo ".env" >> .gitignore
echo "*.pyc" >> .gitignore
echo "__pycache__/" >> .gitignore
echo "logs/" >> .gitignore

# 7. æœ€åˆã®ã‚³ãƒŸãƒƒãƒˆ
git add .
git commit -m "Initial project setup"
```

---

## ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

1. **ã¾ãšå®Ÿè£…ã™ã¹ããƒ•ã‚¡ã‚¤ãƒ«**
   - `requirements.txt`
   - `src/data_collection/academic_collector.py`
   - `src/models/basic_model.py`
   - `test_integration.py`

2. **å‹•ä½œç¢ºèª**
   ```bash
   python test_integration.py
   ```

3. **æ®µéšçš„ãªæ©Ÿèƒ½è¿½åŠ **
   - Phase 1ã‚’å®Œäº†ã—ã¦ã‹ã‚‰æ¬¡ã¸
   - å„ãƒ•ã‚§ãƒ¼ã‚ºã§å‹•ä½œç¢ºèª
   - å•é¡ŒãŒã‚ã‚Œã°ä¿®æ­£ã—ã¦ã‹ã‚‰é€²ã‚€

---

## ğŸ“ ãƒ¡ãƒ¢

- å„ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹ã‚ˆã†ã«è¨­è¨ˆ
- ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ä»–ã®éƒ¨åˆ†ã«å½±éŸ¿ã—ãªã„ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æ§‹æˆ
- å¾ã€…ã«æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¦ã„ãæˆé•·å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- å®Ÿãƒ‡ãƒ¼ã‚¿ãŒæƒã†ã¾ã§ã¯ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å‹•ä½œç¢ºèª

---

*ã“ã®è¨ˆç”»æ›¸ã«å¾“ã£ã¦ã€ä¸€ã¤ãšã¤å®Ÿè£…ã‚’é€²ã‚ã¦ã„ãã¾ã™ã€‚*
*å„ã‚¹ãƒ†ãƒƒãƒ—ã§å‹•ä½œç¢ºèªã‚’ã—ãªãŒã‚‰ã€ç€å®Ÿã«æ§‹ç¯‰ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚*