#!/usr/bin/env python3
import json
import pandas as pd
import numpy as np
from datetime import datetime
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
from pathlib import Path
import warnings
warnings.filterwarnings('ignore')

plt.rcParams['font.sans-serif'] = ['Hiragino Sans', 'Yu Gothic', 'Meiryo', 'Takao', 'IPAexGothic', 'IPAPGothic', 'VL PGothic', 'Noto Sans CJK JP', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def load_data():
    with open('ğŸ“Š_è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹_2024å¹´9æœˆ/ğŸ“‹_ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿/çµ±åˆãƒ‡ãƒ¼ã‚¿ï¼ˆ9,087ä»¶ï¼‰', 'r') as f:
        data = json.load(f)
    return data

def extract_all_papers(data):
    all_papers = []
    for category, subcategories in data.items():
        if isinstance(subcategories, dict):
            for subcategory, papers_list in subcategories.items():
                if isinstance(papers_list, list):
                    for paper in papers_list:
                        paper_copy = paper.copy() if isinstance(paper, dict) else {'title': str(paper)}
                        paper_copy['main_category'] = category
                        paper_copy['sub_category'] = subcategory
                        all_papers.append(paper_copy)
    return all_papers

def analyze_papers(papers):
    df = pd.DataFrame(papers)

    print("\n" + "="*80)
    print("ğŸ“Š è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç·åˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*80)

    print(f"\nğŸ“š ç·è«–æ–‡æ•°: {len(df):,}ä»¶")

    print("\nğŸ“‚ ã‚«ãƒ†ã‚´ãƒªåˆ¥è«–æ–‡æ•°:")
    category_counts = df['main_category'].value_counts()
    for cat, count in category_counts.items():
        print(f"  â€¢ {cat}: {count:,}ä»¶ ({count/len(df)*100:.1f}%)")

    print("\nğŸ”¬ ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªåˆ¥TOP 20:")
    sub_counts = df['sub_category'].value_counts().head(20)
    for sub, count in sub_counts.items():
        print(f"  â€¢ {sub}: {count:,}ä»¶")

    if 'publication_date' in df.columns:
        df['year'] = pd.to_datetime(df['publication_date'], errors='coerce').dt.year
        valid_years = df['year'].dropna()
        if len(valid_years) > 0:
            print(f"\nğŸ“… å‡ºç‰ˆå¹´ã®ç¯„å›²: {int(valid_years.min())} - {int(valid_years.max())}")

            year_counts = valid_years.value_counts().sort_index()
            recent_years = year_counts[year_counts.index >= 2020]
            if len(recent_years) > 0:
                print("\nğŸ“ˆ è¿‘å¹´ã®è«–æ–‡æ•°æ¨ç§» (2020å¹´ä»¥é™):")
                for year, count in recent_years.items():
                    print(f"  â€¢ {int(year)}å¹´: {count:,}ä»¶")

    all_keywords = []
    for keywords_str in df['keywords'].dropna():
        if isinstance(keywords_str, str):
            keywords = [k.strip() for k in keywords_str.split(',') if k.strip()]
            all_keywords.extend(keywords)

    if all_keywords:
        keyword_counter = Counter(all_keywords)
        print(f"\nğŸ”‘ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ:")
        print(f"  â€¢ ç·ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(keyword_counter):,}")
        print(f"  â€¢ å¹³å‡ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°/è«–æ–‡: {len(all_keywords)/len(df):.1f}")

        print("\nğŸ·ï¸ TOP 30 é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
        for keyword, count in keyword_counter.most_common(30):
            print(f"  â€¢ {keyword}: {count:,}å›")

    trend_keywords = ['AI', 'machine learning', 'deep learning', 'personalized',
                     'microbiome', 'sustainability', 'natural', 'organic',
                     'collagen', 'peptide', 'CBD', 'probiotic', 'prebiotic']

    print("\nğŸ”¥ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾çŠ¶æ³:")
    for trend in trend_keywords:
        count = sum(1 for k in all_keywords if trend.lower() in k.lower())
        if count > 0:
            print(f"  â€¢ {trend}: {count}ä»¶")

    return df

def create_visualizations(df):
    print("\nğŸ“Š å¯è¦–åŒ–ã‚’ç”Ÿæˆä¸­...")

    fig = plt.figure(figsize=(20, 12))

    ax1 = plt.subplot(2, 3, 1)
    category_counts = df['main_category'].value_counts()
    colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))
    wedges, texts, autotexts = ax1.pie(category_counts.values,
                                        labels=category_counts.index,
                                        autopct='%1.1f%%',
                                        colors=colors,
                                        startangle=90)
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    ax1.set_title('ãƒ¡ã‚¤ãƒ³ã‚«ãƒ†ã‚´ãƒªåˆ¥è«–æ–‡åˆ†å¸ƒ', fontsize=14, fontweight='bold')

    ax2 = plt.subplot(2, 3, 2)
    top_subs = df['sub_category'].value_counts().head(15)
    ax2.barh(range(len(top_subs)), top_subs.values, color='skyblue')
    ax2.set_yticks(range(len(top_subs)))
    ax2.set_yticklabels([s[:30] + '...' if len(s) > 30 else s for s in top_subs.index])
    ax2.set_xlabel('è«–æ–‡æ•°')
    ax2.set_title('TOP 15 ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)

    for i, v in enumerate(top_subs.values):
        ax2.text(v + 1, i, str(v), va='center')

    if 'year' in df.columns:
        ax3 = plt.subplot(2, 3, 3)
        year_counts = df['year'].value_counts().sort_index()
        recent_years = year_counts[year_counts.index >= 2015]
        if len(recent_years) > 0:
            ax3.plot(recent_years.index, recent_years.values, marker='o', linewidth=2, markersize=8)
            ax3.fill_between(recent_years.index, recent_years.values, alpha=0.3)
            ax3.set_xlabel('å¹´')
            ax3.set_ylabel('è«–æ–‡æ•°')
            ax3.set_title('å¹´åˆ¥è«–æ–‡æ•°ã®æ¨ç§» (2015å¹´ä»¥é™)', fontsize=14, fontweight='bold')
            ax3.grid(True, alpha=0.3)
            ax3.set_xticks(recent_years.index[::2])

    ax4 = plt.subplot(2, 3, 4)
    cat_sub_counts = df.groupby(['main_category', 'sub_category']).size()
    top_combinations = cat_sub_counts.nlargest(20)
    labels = [f"{idx[0][:15]}\n{idx[1][:20]}" for idx in top_combinations.index]
    bars = ax4.bar(range(len(top_combinations)), top_combinations.values, color='coral')
    ax4.set_xticks(range(len(top_combinations)))
    ax4.set_xticklabels(labels, rotation=45, ha='right', fontsize=8)
    ax4.set_ylabel('è«–æ–‡æ•°')
    ax4.set_title('ã‚«ãƒ†ã‚´ãƒªÃ—ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒª TOP 20', fontsize=14, fontweight='bold')
    ax4.grid(True, alpha=0.3)

    ax5 = plt.subplot(2, 3, 5)
    papers_per_sub = df.groupby('sub_category').size()
    ax5.hist(papers_per_sub.values, bins=30, color='lightgreen', edgecolor='darkgreen', alpha=0.7)
    ax5.set_xlabel('è«–æ–‡æ•°')
    ax5.set_ylabel('ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªæ•°')
    ax5.set_title('ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã‚ãŸã‚Šã®è«–æ–‡æ•°åˆ†å¸ƒ', fontsize=14, fontweight='bold')
    ax5.grid(True, alpha=0.3)

    ax6 = plt.subplot(2, 3, 6)
    cat_diversity = df.groupby('main_category')['sub_category'].nunique().sort_values(ascending=False)
    ax6.bar(range(len(cat_diversity)), cat_diversity.values, color='mediumpurple')
    ax6.set_xticks(range(len(cat_diversity)))
    ax6.set_xticklabels(cat_diversity.index, rotation=45, ha='right')
    ax6.set_ylabel('ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªæ•°')
    ax6.set_title('ã‚«ãƒ†ã‚´ãƒªåˆ¥ã®å¤šæ§˜æ€§ï¼ˆã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªæ•°ï¼‰', fontsize=14, fontweight='bold')
    ax6.grid(True, alpha=0.3)

    for i, v in enumerate(cat_diversity.values):
        ax6.text(i, v + 0.5, str(v), ha='center', fontweight='bold')

    plt.suptitle('è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç·åˆåˆ†æãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰', fontsize=16, fontweight='bold', y=1.02)
    plt.tight_layout()

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f'comprehensive_analysis_{timestamp}.png'
    plt.savefig(filename, dpi=150, bbox_inches='tight')
    print(f"  âœ… å¯è¦–åŒ–ã‚’ä¿å­˜: {filename}")

    plt.show()

    return filename

def generate_insights(df):
    print("\nğŸ’¡ ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆ:")

    insights = []

    top_category = df['main_category'].value_counts().index[0]
    top_category_pct = df['main_category'].value_counts().values[0] / len(df) * 100
    insights.append(f"æœ€ã‚‚ç ”ç©¶ãŒæ´»ç™ºãªåˆ†é‡ã¯ã€Œ{top_category}ã€ã§ã€å…¨ä½“ã®{top_category_pct:.1f}%ã‚’å ã‚ã‚‹")

    top_sub = df['sub_category'].value_counts().index[0]
    top_sub_count = df['sub_category'].value_counts().values[0]
    insights.append(f"æœ€ã‚‚è«–æ–‡æ•°ãŒå¤šã„ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªã¯ã€Œ{top_sub}ã€ï¼ˆ{top_sub_count}ä»¶ï¼‰")

    if 'year' in df.columns:
        recent_papers = df[df['year'] >= 2023]
        if len(recent_papers) > 0:
            recent_top_cat = recent_papers['main_category'].value_counts().index[0]
            insights.append(f"2023å¹´ä»¥é™ã®æœ€æ–°ç ”ç©¶ã¯ã€Œ{recent_top_cat}ã€åˆ†é‡ã«é›†ä¸­")

    cat_diversity = df.groupby('main_category')['sub_category'].nunique()
    most_diverse = cat_diversity.idxmax()
    insights.append(f"æœ€ã‚‚å¤šæ§˜ãªç ”ç©¶é ˜åŸŸã¯ã€Œ{most_diverse}ã€ï¼ˆ{cat_diversity[most_diverse]}ã®ã‚µãƒ–ã‚«ãƒ†ã‚´ãƒªï¼‰")

    papers_per_sub = df.groupby('sub_category').size()
    concentrated_subs = papers_per_sub[papers_per_sub >= 100]
    if len(concentrated_subs) > 0:
        insights.append(f"100ä»¶ä»¥ä¸Šã®è«–æ–‡ãŒé›†ä¸­ã—ã¦ã„ã‚‹é ˜åŸŸãŒ{len(concentrated_subs)}å€‹å­˜åœ¨")

    for i, insight in enumerate(insights, 1):
        print(f"  {i}. {insight}")

    return insights

def main():
    print("\nğŸš€ ç·åˆãƒ‡ãƒ¼ã‚¿åˆ†æã‚’é–‹å§‹ã—ã¾ã™...")

    data = load_data()
    papers = extract_all_papers(data)

    df = analyze_papers(papers)

    visualization_file = create_visualizations(df)

    insights = generate_insights(df)

    report = {
        'analysis_date': datetime.now().isoformat(),
        'total_papers': len(df),
        'categories': df['main_category'].value_counts().to_dict(),
        'top_subcategories': df['sub_category'].value_counts().head(20).to_dict(),
        'insights': insights,
        'visualization': visualization_file
    }

    report_file = f'analysis_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜: {report_file}")
    print("\nğŸ‰ åˆ†æãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    main()