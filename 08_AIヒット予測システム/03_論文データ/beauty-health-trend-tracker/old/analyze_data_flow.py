#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»å‡¦ç†ãƒ•ãƒ­ãƒ¼åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦å¯è¦–åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
"""

import json
import glob
from datetime import datetime
from collections import Counter, defaultdict
import os

print("="*70)
print(" ğŸ“Š ç¾å®¹ãƒ»å¥åº·ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼åˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
print("="*70)

# 1. ãƒ‡ãƒ¼ã‚¿åé›†ãƒ•ã‚§ãƒ¼ã‚ºã®åˆ†æ
print("\n" + "="*50)
print("ğŸ“¥ ã€Phase 1: ãƒ‡ãƒ¼ã‚¿åé›†ã€‘")
print("="*50)

# åé›†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®åˆ†æ
raw_files = sorted(glob.glob('data/raw/*.json'))
print(f"\nåé›†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(raw_files)}")

for f in raw_files[-3:]:  # æœ€æ–°3ãƒ•ã‚¡ã‚¤ãƒ«
    with open(f) as fp:
        data = json.load(fp)
        filename = os.path.basename(f)
        total_papers = sum(len(v) if isinstance(v, list) else 0 for v in data.values())

        print(f"\nğŸ“„ {filename}")
        print(f"   ç·è«–æ–‡æ•°: {total_papers}ä»¶")
        print(f"   ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(data)}")

        # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
        if data:
            first_key = list(data.keys())[0]
            if data[first_key] and isinstance(data[first_key], list):
                sample = data[first_key][0]
                print(f"   ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿æ§‹é€ :")
                for key in list(sample.keys())[:5]:
                    value = str(sample[key])[:50] + "..." if len(str(sample[key])) > 50 else str(sample[key])
                    print(f"      - {key}: {value}")

# æœ€æ–°ã®5å¹´åˆ†ãƒ‡ãƒ¼ã‚¿ã‚’è©³ç´°åˆ†æ
five_year_file = 'data/raw/papers_5years_20250924_111234.json'
if os.path.exists(five_year_file):
    print(f"\nğŸ” è©³ç´°åˆ†æ: {os.path.basename(five_year_file)}")
    with open(five_year_file) as fp:
        data = json.load(fp)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥ã®è«–æ–‡æ•°
        print("\n  ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥åé›†çŠ¶æ³:")
        for keyword, papers in data.items():
            if isinstance(papers, list):
                print(f"    â€¢ {keyword}: {len(papers)}ä»¶")

                # å¹´åˆ¥åˆ†å¸ƒ
                year_dist = Counter()
                for paper in papers:
                    if 'publication_date' in paper:
                        year = paper['publication_date'][:4]
                        year_dist[year] += 1

                if year_dist:
                    years_str = ", ".join([f"{y}å¹´:{c}ä»¶" for y, c in sorted(year_dist.items())])
                    print(f"      å¹´åˆ¥: {years_str}")

# 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ã‚§ãƒ¼ã‚ºã®åˆ†æ
print("\n" + "="*50)
print("âš™ï¸ ã€Phase 2: ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã€‘")
print("="*50)

processed_files = sorted(glob.glob('data/processed/*.json'))
print(f"\nå‡¦ç†æ¸ˆã¿ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(processed_files)}")

for f in processed_files:
    filename = os.path.basename(f)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º
    size = os.path.getsize(f)
    size_str = f"{size/1024:.1f}KB" if size < 1024*1024 else f"{size/(1024*1024):.1f}MB"

    try:
        with open(f) as fp:
            data = json.load(fp)
            if isinstance(data, dict):
                total = sum(len(v) if isinstance(v, list) else 0 for v in data.values())

                # AIè¦ç´„ã®æœ‰ç„¡ã‚’ãƒã‚§ãƒƒã‚¯
                has_summary = False
                if data:
                    for papers in data.values():
                        if isinstance(papers, list) and papers:
                            if 'ai_summary' in papers[0]:
                                has_summary = True
                                break

                status = "âœ… AIè¦ç´„ã‚ã‚Š" if has_summary else "ğŸ“„ ç”Ÿãƒ‡ãƒ¼ã‚¿"
                print(f"\nğŸ“„ {filename} ({size_str})")
                print(f"   çŠ¶æ…‹: {status}")
                print(f"   è«–æ–‡æ•°: {total}ä»¶")
    except:
        print(f"\nğŸ“„ {filename} ({size_str}) - èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")

# 3. ãƒ‡ãƒ¼ã‚¿åˆ†æãƒ•ã‚§ãƒ¼ã‚º
print("\n" + "="*50)
print("ğŸ“ˆ ã€Phase 3: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã€‘")
print("="*50)

analysis_files = sorted(glob.glob('data/trends/*.json'))
print(f"\nåˆ†æçµæœãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(analysis_files)}")

for f in analysis_files[-2:]:  # æœ€æ–°2ãƒ•ã‚¡ã‚¤ãƒ«
    filename = os.path.basename(f)

    try:
        with open(f) as fp:
            data = json.load(fp)

            print(f"\nğŸ“Š {filename}")

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®å†…å®¹
            if 'top_keywords' in data:
                print(f"   ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
                for kw in data['top_keywords'][:3]:
                    if isinstance(kw, dict):
                        print(f"      â€¢ {kw.get('keyword', 'N/A')}: é‡è¦åº¦ {kw.get('importance', 'N/A')}")

            if 'ingredient_frequency' in data:
                print(f"   æˆåˆ†é »åº¦åˆ†æ: {len(data['ingredient_frequency'])}ç¨®é¡")
                for ing in data['ingredient_frequency'][:3]:
                    if isinstance(ing, dict):
                        print(f"      â€¢ {ing.get('name', 'N/A')}: {ing.get('count', 0)}å›å‡ºç¾")

            if 'emerging_trends' in data:
                print(f"   æ–°èˆˆãƒˆãƒ¬ãƒ³ãƒ‰: {len(data['emerging_trends'])}å€‹")
    except:
        print(f"\nğŸ“Š {filename} - èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼")

# 4. ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼å›³
print("\n" + "="*70)
print("ğŸ”„ ã€ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ•ãƒ­ãƒ¼å›³ã€‘")
print("="*70)

flow_diagram = """

[1] PubMed API
     â†“
     â†“ æ¤œç´¢ã‚¯ã‚¨ãƒª (10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ Ã— æœ€å¤§15ä»¶/å¹´ Ã— 5å¹´)
     â†“
[2] è«–æ–‡ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿åé›† (papers_5years_*.json)
     â”œâ”€ ã‚¿ã‚¤ãƒˆãƒ«
     â”œâ”€ è‘—è€…
     â”œâ”€ ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆ
     â”œâ”€ ç™ºè¡¨æ—¥
     â”œâ”€ PMID
     â””â”€ URL
     â†“
[3] Gemini AI API
     â†“
     â†“ AIè¦ç´„ãƒ»åˆ†æ
     â†“
[4] è¦ç´„ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ (summarized_*.json)
     â”œâ”€ ä¸»è¦ãªç™ºè¦‹
     â”œâ”€ é‡è¦åº¦ã‚¹ã‚³ã‚¢ (0-10)
     â”œâ”€ é–¢é€£æˆåˆ†
     â””â”€ å¿œç”¨åˆ†é‡
     â†“
[5] ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ (analysis_*.json)
     â”œâ”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥é‡è¦åº¦
     â”œâ”€ æˆåˆ†å‡ºç¾é »åº¦
     â”œâ”€ æ–°èˆˆãƒˆãƒ¬ãƒ³ãƒ‰
     â””â”€ æ™‚ç³»åˆ—åˆ†æ
     â†“
[6] ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¡¨ç¤º
     â”œâ”€ æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ (7æ—¥ã€œ5å¹´)
     â”œâ”€ ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
     â””â”€ 3ã¤ã®è¡¨ç¤ºãƒ¢ãƒ¼ãƒ‰
         â”œâ”€ è¦ç´„ãƒ“ãƒ¥ãƒ¼
         â”œâ”€ å…ƒãƒ‡ãƒ¼ã‚¿ãƒ“ãƒ¥ãƒ¼
         â””â”€ çµ±è¨ˆãƒ“ãƒ¥ãƒ¼
"""

print(flow_diagram)

# 5. APIé€£æºçŠ¶æ³
print("="*70)
print("ğŸ”Œ ã€APIé€£æºçŠ¶æ³ã€‘")
print("="*70)

# ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
from dotenv import load_dotenv
load_dotenv()

apis = {
    "PubMed (NCBI)": {
        "key": os.getenv('NCBI_API_KEY'),
        "status": "âœ… è¨­å®šæ¸ˆã¿" if os.getenv('NCBI_API_KEY') and os.getenv('NCBI_API_KEY') != '' else "âš ï¸ æœªè¨­å®šï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚ã‚Šï¼‰",
        "ç”¨é€”": "è«–æ–‡æ¤œç´¢ãƒ»ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿å–å¾—"
    },
    "Google Gemini AI": {
        "key": os.getenv('GEMINI_API_KEY'),
        "status": "âœ… è¨­å®šæ¸ˆã¿" if os.getenv('GEMINI_API_KEY') and os.getenv('GEMINI_API_KEY') != 'your_gemini_api_key_here' else "âŒ æœªè¨­å®š",
        "ç”¨é€”": "è«–æ–‡è¦ç´„ãƒ»ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"
    }
}

for api_name, info in apis.items():
    print(f"\n{api_name}:")
    print(f"  çŠ¶æ…‹: {info['status']}")
    print(f"  ç”¨é€”: {info['ç”¨é€”']}")

# 6. ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æ
print("\n" + "="*70)
print("ğŸ“Š ã€ãƒ‡ãƒ¼ã‚¿å“è³ªåˆ†æã€‘")
print("="*70)

if os.path.exists(five_year_file):
    with open(five_year_file) as fp:
        data = json.load(fp)

        total_papers = sum(len(v) if isinstance(v, list) else 0 for v in data.values())

        # ãƒ‡ãƒ¼ã‚¿ã®å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        complete_count = 0
        missing_fields = defaultdict(int)

        for papers in data.values():
            if isinstance(papers, list):
                for paper in papers:
                    # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãƒã‚§ãƒƒã‚¯
                    required = ['title', 'abstract', 'authors', 'publication_date', 'pmid']
                    is_complete = True
                    for field in required:
                        if field not in paper or not paper[field]:
                            missing_fields[field] += 1
                            is_complete = False
                    if is_complete:
                        complete_count += 1

        print(f"\nãƒ‡ãƒ¼ã‚¿å®Œå…¨æ€§:")
        print(f"  å®Œå…¨ãªãƒ¬ã‚³ãƒ¼ãƒ‰: {complete_count}/{total_papers} ({complete_count*100/total_papers:.1f}%)")

        if missing_fields:
            print(f"\n  æ¬ æãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰:")
            for field, count in missing_fields.items():
                print(f"    â€¢ {field}: {count}ä»¶")

print("\n" + "="*70)
print("âœ… åˆ†æå®Œäº†")
print("="*70)