#!/usr/bin/env python3
"""
ãƒãƒƒãƒåˆ†æã‚·ã‚¹ãƒ†ãƒ 
å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ®µéšçš„ã«Gemini APIã§åˆ†æ
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
import google.generativeai as genai
import argparse

sys.path.append(str(Path(__file__).parent.parent))

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATABASE_DIR = Path('./database')
ANALYSIS_DIR = DATABASE_DIR / 'analysis'
ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)

class BatchAnalysisSystem:
    """ãƒãƒƒãƒåˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        # Gemini APIè¨­å®š
        self.api_key = os.getenv('GEMINI_API_KEY')
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-1.5-flash')

        self.master_db = DATABASE_DIR / 'master_papers.json'
        self.analysis_db = ANALYSIS_DIR / 'analyzed_papers.json'
        self.analysis_meta = ANALYSIS_DIR / 'analysis_metadata.json'

    def analyze_batch(self, batch_size=50, max_batches=None):
        """
        ãƒãƒƒãƒå˜ä½ã§è«–æ–‡ã‚’åˆ†æ

        Args:
            batch_size: 1ãƒãƒƒãƒã‚ãŸã‚Šã®è«–æ–‡æ•°
            max_batches: æœ€å¤§ãƒãƒƒãƒæ•°ï¼ˆNoneã§å…¨ã¦ï¼‰
        """
        if not self.api_key:
            print("âŒ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return

        # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹èª­ã¿è¾¼ã¿
        if not self.master_db.exists():
            print("âŒ ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        with open(self.master_db, 'r', encoding='utf-8') as f:
            all_papers = json.load(f)

        # æ—¢ã«åˆ†ææ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        if self.analysis_db.exists():
            with open(self.analysis_db, 'r', encoding='utf-8') as f:
                analyzed_papers = json.load(f)
        else:
            analyzed_papers = {}

        # æœªåˆ†æã®è«–æ–‡ã‚’æŠ½å‡º
        unanalyzed = {
            pid: paper for pid, paper in all_papers.items()
            if pid not in analyzed_papers
        }

        if not unanalyzed:
            print("âœ… ã™ã¹ã¦ã®è«–æ–‡ãŒåˆ†ææ¸ˆã¿ã§ã™")
            return

        print("="*70)
        print("ğŸ¤– ãƒãƒƒãƒåˆ†æã‚’é–‹å§‹ã—ã¾ã™")
        print(f"ç·è«–æ–‡æ•°: {len(all_papers):,}ä»¶")
        print(f"åˆ†ææ¸ˆã¿: {len(analyzed_papers):,}ä»¶")
        print(f"æœªåˆ†æ: {len(unanalyzed):,}ä»¶")
        print(f"ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}ä»¶")
        print("="*70)

        # ãƒãƒƒãƒå‡¦ç†
        paper_ids = list(unanalyzed.keys())
        total_batches = (len(paper_ids) + batch_size - 1) // batch_size

        if max_batches:
            total_batches = min(total_batches, max_batches)

        analyzed_count = 0
        error_count = 0

        for batch_num in range(total_batches):
            batch_start = batch_num * batch_size
            batch_end = min(batch_start + batch_size, len(paper_ids))
            batch_ids = paper_ids[batch_start:batch_end]

            print(f"\n[ãƒãƒƒãƒ {batch_num + 1}/{total_batches}] "
                  f"{len(batch_ids)}ä»¶ã‚’åˆ†æä¸­...")

            for idx, paper_id in enumerate(batch_ids, 1):
                paper = unanalyzed[paper_id]

                try:
                    # è«–æ–‡ã‚’åˆ†æ
                    analysis = self._analyze_single_paper(paper)

                    if analysis:
                        # åˆ†æçµæœã‚’ä¿å­˜
                        analyzed_papers[paper_id] = {
                            **paper,
                            'ai_analysis': analysis,
                            'analyzed_at': datetime.now().isoformat()
                        }
                        analyzed_count += 1

                        if idx % 10 == 0:
                            print(f"  é€²æ—: {idx}/{len(batch_ids)}")

                    # APIåˆ¶é™å¯¾ç­–ï¼ˆ1ç§’å¾…æ©Ÿï¼‰
                    time.sleep(1)

                except Exception as e:
                    print(f"  âŒ ã‚¨ãƒ©ãƒ¼ (ID: {paper_id}): {e}")
                    error_count += 1
                    continue

            # ãƒãƒƒãƒå®Œäº†å¾Œã«ä¿å­˜
            self._save_analyzed_papers(analyzed_papers)
            print(f"  âœ… ãƒãƒƒãƒ{batch_num + 1}å®Œäº†: {analyzed_count}ä»¶åˆ†ææ¸ˆã¿")

            # ãƒãƒƒãƒé–“ã®ä¼‘æ†©ï¼ˆ10ç§’ï¼‰
            if batch_num < total_batches - 1:
                print("  ğŸ’¤ 10ç§’å¾…æ©Ÿä¸­...")
                time.sleep(10)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        self._update_analysis_metadata({
            'last_analysis': datetime.now().isoformat(),
            'total_analyzed': len(analyzed_papers),
            'latest_batch_analyzed': analyzed_count,
            'errors': error_count
        })

        print("\n" + "="*70)
        print("âœ… ãƒãƒƒãƒåˆ†æå®Œäº†!")
        print(f"æ–°è¦åˆ†æ: {analyzed_count}ä»¶")
        print(f"ç·åˆ†ææ¸ˆã¿: {len(analyzed_papers)}ä»¶")
        print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
        print("="*70)

    def _analyze_single_paper(self, paper):
        """å˜ä¸€è«–æ–‡ã‚’åˆ†æ"""
        prompt = f"""
        ä»¥ä¸‹ã®è«–æ–‡ã‚’åˆ†æã—ã€ç¾å®¹ãƒ»å¥åº·ãƒˆãƒ¬ãƒ³ãƒ‰ã®è¦³ç‚¹ã‹ã‚‰è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

        ã‚¿ã‚¤ãƒˆãƒ«: {paper.get('title', 'N/A')}
        è¦ç´„: {paper.get('abstract', 'N/A')[:1000]}
        ç™ºè¡¨æ—¥: {paper.get('publication_date', 'N/A')}

        ä»¥ä¸‹ã®é …ç›®ã‚’æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ï¼š
        1. ä¸»è¦ãªç™ºè¦‹ï¼ˆ2-3ç‚¹ï¼‰
        2. ç¾å®¹ãƒ»å¥åº·ç”£æ¥­ã¸ã®å½±éŸ¿åº¦ï¼ˆ1-10ç‚¹ï¼‰
        3. å®Ÿç”¨åŒ–ã®å¯èƒ½æ€§ï¼ˆé«˜/ä¸­/ä½ï¼‰
        4. é–¢é€£ã™ã‚‹ç¾å®¹æˆåˆ†ãƒ»æŠ€è¡“ï¼ˆæœ€å¤§3ã¤ï¼‰
        5. æ³¨ç›®ãƒã‚¤ãƒ³ãƒˆï¼ˆ1æ–‡ï¼‰
        """

        try:
            response = self.model.generate_content(prompt)
            return self._parse_analysis(response.text)
        except Exception as e:
            print(f"    åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _parse_analysis(self, text):
        """åˆ†æçµæœã‚’ãƒ‘ãƒ¼ã‚¹"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªæ§‹é€ åŒ–ï¼ˆå®Ÿéš›ã®å¿œç­”ã«å¿œã˜ã¦èª¿æ•´ï¼‰
        return {
            'summary': text[:500],  # æœ€åˆã®500æ–‡å­—ã‚’è¦ç´„ã¨ã—ã¦ä¿å­˜
            'full_analysis': text,
            'analyzed_at': datetime.now().isoformat()
        }

    def _save_analyzed_papers(self, data):
        """åˆ†ææ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        with open(self.analysis_db, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _update_analysis_metadata(self, updates):
        """åˆ†æãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        if self.analysis_meta.exists():
            with open(self.analysis_meta, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {}

        metadata.update(updates)

        with open(self.analysis_meta, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def generate_trend_report(self):
        """ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        if not self.analysis_db.exists():
            print("âŒ åˆ†æãƒ‡ãƒ¼ã‚¿ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            return

        with open(self.analysis_db, 'r', encoding='utf-8') as f:
            analyzed_papers = json.load(f)

        print("\n" + "="*70)
        print("ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("="*70)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥é›†è¨ˆ
        keyword_stats = {}
        for paper_id, paper in analyzed_papers.items():
            for keyword in paper.get('keywords', []):
                if keyword not in keyword_stats:
                    keyword_stats[keyword] = {
                        'count': 0,
                        'papers': []
                    }
                keyword_stats[keyword]['count'] += 1
                keyword_stats[keyword]['papers'].append(paper_id)

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        report = {
            'generated_at': datetime.now().isoformat(),
            'total_papers': len(analyzed_papers),
            'keyword_statistics': keyword_stats,
            'top_keywords': sorted(keyword_stats.items(),
                                  key=lambda x: x[1]['count'],
                                  reverse=True)[:10]
        }

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        report_file = ANALYSIS_DIR / f"trend_report_{datetime.now().strftime('%Y%m%d')}.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"âœ… ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
        print(f"\nãƒˆãƒƒãƒ—10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:")
        for keyword, stats in report['top_keywords']:
            print(f"  â€¢ {keyword}: {stats['count']}ä»¶")

        return report


def main():
    parser = argparse.ArgumentParser(description='ãƒãƒƒãƒåˆ†æã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--mode', choices=['analyze', 'report', 'status'],
                       default='status', help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--batch-size', type=int, default=50,
                       help='ãƒãƒƒãƒã‚µã‚¤ã‚º')
    parser.add_argument('--max-batches', type=int, default=None,
                       help='æœ€å¤§ãƒãƒƒãƒæ•°')

    args = parser.parse_args()

    system = BatchAnalysisSystem()

    if args.mode == 'analyze':
        # ãƒãƒƒãƒåˆ†æå®Ÿè¡Œ
        system.analyze_batch(
            batch_size=args.batch_size,
            max_batches=args.max_batches
        )

    elif args.mode == 'report':
        # ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        system.generate_trend_report()

    elif args.mode == 'status':
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹è¡¨ç¤º
        if system.master_db.exists():
            with open(system.master_db, 'r', encoding='utf-8') as f:
                all_papers = json.load(f)
            total = len(all_papers)
        else:
            total = 0

        if system.analysis_db.exists():
            with open(system.analysis_db, 'r', encoding='utf-8') as f:
                analyzed = json.load(f)
            analyzed_count = len(analyzed)
        else:
            analyzed_count = 0

        print("\nğŸ“Š åˆ†æã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹")
        print("="*50)
        print(f"ç·è«–æ–‡æ•°: {total:,}ä»¶")
        print(f"åˆ†ææ¸ˆã¿: {analyzed_count:,}ä»¶")
        print(f"æœªåˆ†æ: {total - analyzed_count:,}ä»¶")

        if total > 0:
            progress = (analyzed_count / total) * 100
            print(f"é€²æ—: {progress:.1f}%")


if __name__ == "__main__":
    main()