#!/usr/bin/env python3
"""
5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.collectors.pubmed_collector import PubMedCollector
from src.analyzers.paper_summarizer import PaperSummarizer

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

def collect_5years_data():
    """5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†"""

    print("ğŸš€ 5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™")
    print("="*50)

    # è¨­å®š
    data_dir = Path('./data')
    (data_dir / "raw").mkdir(parents=True, exist_ok=True)
    (data_dir / "processed").mkdir(parents=True, exist_ok=True)
    (data_dir / "trends").mkdir(parents=True, exist_ok=True)

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®š
    keywords = [
        "NMN anti-aging",
        "collagen supplement skin",
        "hyaluronic acid hydration",
        "retinol skincare",
        "CBD inflammation",
        "exosome regeneration",
        "peptide anti-aging",
        "niacinamide brightening",
        "ceramide barrier",
        "vitamin C antioxidant"
    ]

    # APIè¨­å®š
    ncbi_api_key = os.getenv('NCBI_API_KEY')
    gemini_api_key = os.getenv('GEMINI_API_KEY')

    if not gemini_api_key:
        print("âŒ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return

    # æœŸé–“ã”ã¨ã«ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆ1å¹´ãšã¤5å›ï¼‰
    collector = PubMedCollector(api_key=ncbi_api_key)
    all_papers = {}

    for year in range(5):
        print(f"\nğŸ“… {year}å¹´å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ä¸­...")

        # æ—¥ä»˜ç¯„å›²ã‚’è¨ˆç®—
        end_date = datetime(2024, 9, 24) - timedelta(days=365*year)
        start_date = end_date - timedelta(days=365)

        year_papers = {}

        for idx, keyword in enumerate(keywords, 1):
            print(f"  [{idx}/{len(keywords)}] {keyword}", end=" ")

            # å„å¹´ã§æœ€å¤§3ä»¶ãšã¤åé›†ï¼ˆåˆè¨ˆ150ä»¶ã«æŠ‘ãˆã‚‹ï¼‰
            papers = collector.search_and_fetch(
                keyword,
                max_results=3,
                start_date=start_date,
                end_date=end_date
            )

            print(f"â†’ {len(papers)}ä»¶")

            if papers:
                if keyword not in all_papers:
                    all_papers[keyword] = []
                all_papers[keyword].extend(papers)

            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(0.5)

    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    raw_file = data_dir / "raw" / f"papers_5years_{timestamp}.json"

    with open(raw_file, 'w', encoding='utf-8') as f:
        json.dump(all_papers, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… åé›†å®Œäº†: {sum(len(papers) for papers in all_papers.values())}ä»¶ã®è«–æ–‡")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {raw_file}")

    # Gemini APIã§è¦ç´„
    print("\nğŸ¤– AIè¦ç´„å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...")
    summarizer = PaperSummarizer(api_key=gemini_api_key)
    summarized_data = {}

    for keyword, papers in all_papers.items():
        if papers:
            print(f"è¦ç´„ä¸­: {keyword} ({len(papers)}ä»¶)")
            # æœ€å¤§10ä»¶ã¾ã§è¦ç´„
            summarized = summarizer.batch_summarize(papers[:10], max_papers=10)
            summarized_data[keyword] = summarized

    # è¦ç´„ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    processed_file = data_dir / "processed" / f"summarized_5years_{timestamp}.json"
    with open(processed_file, 'w', encoding='utf-8') as f:
        json.dump(summarized_data, f, ensure_ascii=False, indent=2)

    # latest_papers.jsonã¨ã—ã¦ä¿å­˜ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰
    latest_file = data_dir / "processed" / "latest_papers.json"
    with open(latest_file, 'w', encoding='utf-8') as f:
        json.dump(summarized_data, f, ensure_ascii=False, indent=2)

    print(f"âœ… è¦ç´„å®Œäº†: {processed_file}")

    # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
    print("\nğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æä¸­...")
    trend_analysis = summarizer.analyze_trends(summarized_data)

    analysis_file = data_dir / "trends" / f"analysis_5years_{timestamp}.json"
    with open(analysis_file, 'w', encoding='utf-8') as f:
        json.dump(trend_analysis, f, ensure_ascii=False, indent=2)

    # latest_analysis.jsonã¨ã—ã¦ä¿å­˜
    latest_analysis = data_dir / "trends" / "latest_analysis.json"
    with open(latest_analysis, 'w', encoding='utf-8') as f:
        json.dump(trend_analysis, f, ensure_ascii=False, indent=2)

    print(f"âœ… åˆ†æå®Œäº†: {analysis_file}")
    print("\nğŸ‰ 5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

if __name__ == "__main__":
    # PubMedCollectorã®ä¿®æ­£ãŒå¿…è¦
    # search_and_fetchãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 

    # ã¾ãšã€PubMedCollectorã«ãƒ¡ã‚½ãƒƒãƒ‰ã‚’è¿½åŠ 
    import inspect
    from src.collectors.pubmed_collector import PubMedCollector

    # ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯è¿½åŠ 
    if not hasattr(PubMedCollector, 'search_and_fetch'):
        def search_and_fetch(self, query, max_results=10, start_date=None, end_date=None):
            """æ¤œç´¢ã¨å–å¾—ã‚’ä¸€æ‹¬ã§è¡Œã†"""
            # æ—¥ä»˜ç¯„å›²ã®è¨ˆç®—
            if end_date is None:
                end_date = datetime(2024, 9, 24)
            if start_date is None:
                start_date = end_date - timedelta(days=30)

            days_back = (end_date - start_date).days

            # æ¤œç´¢
            pmids = self.search_papers(query, max_results, days_back)

            # è©³ç´°å–å¾—
            if pmids:
                papers = self.fetch_paper_details(pmids)
                return papers
            return []

        # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ 
        PubMedCollector.search_and_fetch = search_and_fetch

    # ãƒ‡ãƒ¼ã‚¿åé›†ã‚’å®Ÿè¡Œ
    collect_5years_data()