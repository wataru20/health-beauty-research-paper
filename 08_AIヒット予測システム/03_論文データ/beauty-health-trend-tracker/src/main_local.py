#!/usr/bin/env python3
"""
ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç’°å¢ƒå¤‰æ•°å¯¾å¿œãƒ»å®šæœŸå®Ÿè¡Œæ©Ÿèƒ½ä»˜ã
"""

import os
import sys
import json
import time
import schedule
import argparse
import logging
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.collectors.pubmed_collector import PubMedCollector
from src.analyzers.paper_summarizer import PaperSummarizer

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/tracker.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class LocalTrendTracker:
    """ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒç”¨ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent
        self.config_path = self.project_root / "configs" / "keywords.json"
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.data_dir = Path(os.getenv('DATA_DIR', './data'))
        self.backup_dir = Path(os.getenv('BACKUP_DIR', './backups'))
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        (self.data_dir / "raw").mkdir(parents=True, exist_ok=True)
        (self.data_dir / "processed").mkdir(parents=True, exist_ok=True)
        (self.data_dir / "trends").mkdir(parents=True, exist_ok=True)
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        Path("logs").mkdir(exist_ok=True)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿
        self.config = self._load_config()
        
        # API ã‚­ãƒ¼å–å¾—
        self.ncbi_api_key = os.getenv('NCBI_API_KEY')
        self.gemini_api_key = os.getenv('GEMINI_API_KEY')
        
        if not self.gemini_api_key:
            logger.warning("âš ï¸ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚.envãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®š
        self.days_back = int(os.getenv('DAYS_BACK', 30))
        self.max_papers = int(os.getenv('MAX_PAPERS_PER_KEYWORD', 5))
    
    def _load_config(self) -> dict:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
            return {}
    
    def _get_priority_keywords(self) -> list:
        """å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å–å¾—"""
        # ãƒãƒ¼ãƒ‰ã‚³ãƒ¼ãƒ‰ã•ã‚ŒãŸå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè‹±èªï¼‰
        priority_keywords_en = [
            "NMN anti-aging",
            "collagen supplement skin",
            "hyaluronic acid hydration",
            "retinol skincare",
            "CBD inflammation",
            "exosome regeneration",
            "peptide anti-aging",
            "niacinamide brightening",
            "ceramide barrier",
            "vitamin C antioxidant"
        ]
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¥æœ¬èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚‚å–å¾—
        priority_keywords_jp = []
        if self.config:
            # æ–°èˆˆæˆåˆ†ã‚’å„ªå…ˆ
            if 'ingredients' in self.config and 'emerging' in self.config['ingredients']:
                priority_keywords_jp.extend(self.config['ingredients']['emerging'].get('keywords', [])[:5])
        
        return priority_keywords_en[:5]  # ç„¡æ–™æ ã‚’è€ƒæ…®ã—ã¦5å€‹ã¾ã§
    
    def collect_papers(self):
        """è«–æ–‡ã‚’åé›†"""
        logger.info("=" * 50)
        logger.info("ğŸ“š è«–æ–‡åé›†ã‚’é–‹å§‹ã—ã¾ã™")
        logger.info(f"æœŸé–“: éå»{self.days_back}æ—¥é–“")
        logger.info(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§: {self.max_papers}ä»¶")
        
        # ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼åˆæœŸåŒ–
        collector = PubMedCollector(api_key=self.ncbi_api_key)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—
        keywords = self._get_priority_keywords()
        logger.info(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {keywords}")
        
        # è«–æ–‡åé›†
        results = collector.collect_papers_for_keywords(
            keywords,
            max_per_keyword=self.max_papers,
            days_back=self.days_back
        )
        
        # çµæœã‚’ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = self.data_dir / "raw" / f"papers_{timestamp}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        total_papers = sum(len(papers) for papers in results.values())
        logger.info(f"âœ… åé›†å®Œäº†: {total_papers}ä»¶ã®è«–æ–‡")
        logger.info(f"ä¿å­˜å…ˆ: {output_path}")
        
        return output_path
    
    def analyze_papers(self):
        """è«–æ–‡ã‚’åˆ†æãƒ»è¦ç´„"""
        logger.info("=" * 50)
        logger.info("ğŸ”¬ è«–æ–‡åˆ†æã‚’é–‹å§‹ã—ã¾ã™")
        
        if not self.gemini_api_key:
            logger.error("âŒ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            return None
        
        # æœ€æ–°ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        raw_files = sorted(
            (self.data_dir / "raw").glob("papers_*.json"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )
        
        if not raw_files:
            logger.error("âŒ åˆ†æã™ã‚‹è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        latest_file = raw_files[0]
        logger.info(f"åˆ†æå¯¾è±¡: {latest_file.name}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(latest_file, 'r', encoding='utf-8') as f:
            papers_data = json.load(f)
        
        # è¦ç´„å™¨åˆæœŸåŒ–
        summarizer = PaperSummarizer(api_key=self.gemini_api_key)
        
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è«–æ–‡ã‚’è¦ç´„
        summarized_data = {}
        
        for keyword, papers in papers_data.items():
            if papers:
                logger.info(f"è¦ç´„ä¸­: {keyword}")
                summarized = summarizer.batch_summarize(papers, max_papers=3)
                summarized_data[keyword] = summarized
        
        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        processed_path = self.data_dir / "processed" / f"summarized_{timestamp}.json"
        
        with open(processed_path, 'w', encoding='utf-8') as f:
            json.dump(summarized_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… è¦ç´„å®Œäº†: {processed_path}")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        logger.info("ğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æä¸­...")
        trend_analysis = summarizer.analyze_trends(summarized_data)
        
        # åˆ†æçµæœã‚’ä¿å­˜
        analysis_path = self.data_dir / "trends" / f"analysis_{timestamp}.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(trend_analysis, f, ensure_ascii=False, indent=2)
        
        # æœ€æ–°ç‰ˆã¨ã—ã¦ã‚‚ä¿å­˜ï¼ˆãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ï¼‰
        latest_analysis = self.data_dir / "trends" / "latest_analysis.json"
        latest_papers = self.data_dir / "processed" / "latest_papers.json"
        
        with open(latest_analysis, 'w', encoding='utf-8') as f:
            json.dump(trend_analysis, f, ensure_ascii=False, indent=2)
        
        with open(latest_papers, 'w', encoding='utf-8') as f:
            json.dump(summarized_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… åˆ†æå®Œäº†: {analysis_path}")
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆè¡¨ç¤º
        logger.info("\nğŸ’¡ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for insight in trend_analysis.get('trend_insights', []):
            logger.info(f"  â€¢ {insight}")
        
        return analysis_path
    
    def backup_data(self):
        """ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—"""
        logger.info("ğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­...")
        
        timestamp = datetime.now().strftime('%Y%m%d')
        backup_path = self.backup_dir / f"backup_{timestamp}"
        backup_path.mkdir(exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚³ãƒ”ãƒ¼
        import shutil
        for subdir in ['raw', 'processed', 'trends']:
            src = self.data_dir / subdir
            dst = backup_path / subdir
            if src.exists():
                shutil.copytree(src, dst, dirs_exist_ok=True)
        
        logger.info(f"âœ… ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å®Œäº†: {backup_path}")
    
    def run_full_cycle(self):
        """ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œ"""
        logger.info("\n" + "=" * 50)
        logger.info("ğŸš€ ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Ÿè¡Œé–‹å§‹")
        logger.info(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        try:
            # 1. è«–æ–‡åé›†
            self.collect_papers()
            
            # 2. åˆ†æãƒ»è¦ç´„
            self.analyze_papers()
            
            # 3. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ï¼ˆé€±1å›ï¼‰
            if datetime.now().weekday() == 0:  # æœˆæ›œæ—¥
                self.backup_data()
            
            logger.info("âœ… ãƒ•ãƒ«ã‚µã‚¤ã‚¯ãƒ«å®Œäº†")
            
        except Exception as e:
            logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise
    
    def schedule_jobs(self, schedule_type='weekly'):
        """å®šæœŸå®Ÿè¡Œã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"""
        if schedule_type == 'weekly':
            # æ¯é€±æœˆæ›œæ—¥ 9:00
            schedule.every().monday.at("09:00").do(self.run_full_cycle)
            logger.info("ğŸ“… æ¯é€±æœˆæ›œæ—¥ 9:00 ã«å®Ÿè¡Œã™ã‚‹ã‚ˆã†è¨­å®šã—ã¾ã—ãŸ")
        
        elif schedule_type == 'daily':
            # æ¯æ—¥ 9:00
            schedule.every().day.at("09:00").do(self.run_full_cycle)
            logger.info("ğŸ“… æ¯æ—¥ 9:00 ã«å®Ÿè¡Œã™ã‚‹ã‚ˆã†è¨­å®šã—ã¾ã—ãŸ")
        
        elif schedule_type == 'hourly':
            # æ¯æ™‚ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
            schedule.every().hour.do(self.run_full_cycle)
            logger.info("ğŸ“… æ¯æ™‚å®Ÿè¡Œã™ã‚‹ã‚ˆã†è¨­å®šã—ã¾ã—ãŸ")
    
    def run_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ"""
        logger.info("â° ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹ã—ã¾ã—ãŸ")
        logger.info("Ctrl+C ã§çµ‚äº†ã—ã¾ã™")
        
        while True:
            try:
                schedule.run_pending()
                time.sleep(60)  # 1åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
            except KeyboardInterrupt:
                logger.info("\nğŸ‘‹ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’çµ‚äº†ã—ã¾ã™")
                break


def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description='ç¾å®¹ãƒ»å¥åº·æˆåˆ†ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ç‰ˆï¼‰'
    )
    
    parser.add_argument(
        '--mode',
        choices=['collect', 'analyze', 'all', 'schedule'],
        default='all',
        help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰'
    )
    
    parser.add_argument(
        '--schedule-type',
        choices=['weekly', 'daily', 'hourly'],
        default='weekly',
        help='ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®Ÿè¡Œã®é »åº¦'
    )
    
    args = parser.parse_args()
    
    # ãƒˆãƒ©ãƒƒã‚«ãƒ¼åˆæœŸåŒ–
    tracker = LocalTrendTracker()
    
    if args.mode == 'collect':
        tracker.collect_papers()
    
    elif args.mode == 'analyze':
        tracker.analyze_papers()
    
    elif args.mode == 'all':
        tracker.run_full_cycle()
    
    elif args.mode == 'schedule':
        # åˆå›å®Ÿè¡Œ
        tracker.run_full_cycle()
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼é–‹å§‹
        tracker.schedule_jobs(args.schedule_type)
        tracker.run_scheduler()


if __name__ == "__main__":
    main()
