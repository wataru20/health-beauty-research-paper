#!/usr/bin/env python3
"""
æœˆæ¬¡å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›†ãƒ»ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
åˆå›ã¯10å¹´åˆ†ã®å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã€ä»¥é™ã¯æœˆæ¬¡ã§æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿è¿½åŠ 
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
import argparse
import hashlib

sys.path.append(str(Path(__file__).parent.parent))
from src.collectors.pubmed_collector import PubMedCollector

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATABASE_DIR = Path('./database')
DATABASE_DIR.mkdir(exist_ok=True)

class MonthlyCollectionSystem:
    """æœˆæ¬¡åé›†ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        self.collector = PubMedCollector(api_key=os.getenv('NCBI_API_KEY'))
        self.master_db = DATABASE_DIR / 'master_papers.json'
        self.meta_db = DATABASE_DIR / 'metadata.json'
        self.monthly_dir = DATABASE_DIR / 'monthly_updates'
        self.monthly_dir.mkdir(exist_ok=True)

        # æ‹¡å¼µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆå…¨60ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼‰
        self.all_keywords = self._get_all_keywords()

    def _get_all_keywords(self):
        """å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        return [
            # Core (10)
            "NMN anti-aging", "collagen supplement skin", "hyaluronic acid hydration",
            "retinol skincare", "CBD inflammation", "exosome regeneration",
            "peptide anti-aging", "niacinamide brightening", "ceramide barrier",
            "vitamin C antioxidant",

            # Skincare ingredients (10)
            "retinoid wrinkle", "salicylic acid acne", "glycolic acid exfoliation",
            "lactic acid moisturizing", "vitamin E skin protection",
            "resveratrol antioxidant skin", "bakuchiol retinol alternative",
            "tranexamic acid melasma", "azelaic acid rosacea", "centella asiatica healing",

            # Anti-aging (10)
            "NAD+ longevity", "sirtuin activation aging", "telomerase anti-aging",
            "growth factor skin rejuvenation", "stem cell cosmetics",
            "autophagy skin aging", "senolytic compounds",
            "mitochondrial dysfunction aging", "oxidative stress skin aging",
            "collagen synthesis stimulation",

            # Beauty supplements (10)
            "biotin hair growth", "omega-3 skin health", "probiotics skin microbiome",
            "astaxanthin UV protection", "coenzyme Q10 wrinkle",
            "glutathione skin whitening", "zinc acne treatment",
            "vitamin D skin health", "curcumin anti-inflammatory skin",
            "green tea polyphenols skin",

            # Advanced treatments (10)
            "microneedling collagen induction", "LED therapy skin rejuvenation",
            "radiofrequency skin tightening", "platelet-rich plasma facial",
            "mesotherapy skin treatment", "cryotherapy anti-aging",
            "ultrasound skin therapy", "laser resurfacing wrinkle",
            "chemical peel rejuvenation", "dermabrasion skin texture",

            # Natural ingredients (10)
            "aloe vera skin healing", "tea tree oil acne", "rosehip oil scar treatment",
            "jojoba oil moisturizer", "argan oil anti-aging",
            "manuka honey wound healing", "turmeric skin inflammation",
            "ginseng skin vitality", "licorice root pigmentation",
            "chamomile sensitive skin"
        ]

    def initial_massive_collection(self, years=10, papers_per_keyword=100):
        """
        åˆå›ã®å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆ10å¹´åˆ†ï¼‰

        Args:
            years: é¡ã‚‹å¹´æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10å¹´ï¼‰
            papers_per_keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§è«–æ–‡æ•°
        """
        print("="*70)
        print("ğŸ“š åˆå›å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™")
        print(f"æœŸé–“: éå»{years}å¹´")
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(self.all_keywords)}")
        print(f"æœ€å¤§äºˆæƒ³è«–æ–‡æ•°: {len(self.all_keywords) * papers_per_keyword}")
        print("="*70)

        # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        if self.master_db.exists():
            print("\nâš ï¸ æ—¢å­˜ã®ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã™")
            with open(self.master_db, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                print(f"  æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data)}ä»¶")
        else:
            existing_data = {}

        # åé›†æœŸé–“è¨­å®š
        end_date = datetime.now()
        start_date = end_date - timedelta(days=365 * years)

        collected_count = 0
        error_count = 0

        print(f"\nåé›†é–‹å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        for idx, keyword in enumerate(self.all_keywords, 1):
            print(f"\n[{idx}/{len(self.all_keywords)}] {keyword}")

            try:
                # è«–æ–‡æ¤œç´¢
                papers = self.collector.search_and_fetch(
                    keyword,
                    max_results=papers_per_keyword,
                    start_date=start_date,
                    end_date=end_date
                )

                if papers:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ã¨ãƒãƒ¼ã‚¸
                    new_papers = 0
                    for paper in papers:
                        paper_id = self._generate_paper_id(paper)
                        if paper_id not in existing_data:
                            existing_data[paper_id] = paper
                            existing_data[paper_id]['keywords'] = [keyword]
                            existing_data[paper_id]['collected_at'] = datetime.now().isoformat()
                            new_papers += 1
                        elif keyword not in existing_data[paper_id].get('keywords', []):
                            existing_data[paper_id]['keywords'].append(keyword)

                    collected_count += new_papers
                    print(f"  âœ… {len(papers)}ä»¶å–å¾— (æ–°è¦: {new_papers}ä»¶)")
                else:
                    print(f"  âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")

                # 10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ã«ä¿å­˜
                if idx % 10 == 0:
                    self._save_master_database(existing_data)
                    print(f"  ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ (ç·æ•°: {len(existing_data)}ä»¶)")

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(0.1)  # APIã‚­ãƒ¼ã‚ã‚Šãªã®ã§é«˜é€Ÿ

            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                error_count += 1
                continue

        # æœ€çµ‚ä¿å­˜
        self._save_master_database(existing_data)

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
        self._update_metadata({
            'last_full_collection': datetime.now().isoformat(),
            'total_papers': len(existing_data),
            'total_keywords': len(self.all_keywords),
            'collection_years': years,
            'errors': error_count
        })

        print("\n" + "="*70)
        print("âœ… åˆå›å¤§é‡åé›†å®Œäº†ï¼")
        print(f"ç·è«–æ–‡æ•°: {len(existing_data)}ä»¶")
        print(f"æ–°è¦è¿½åŠ : {collected_count}ä»¶")
        print(f"ã‚¨ãƒ©ãƒ¼: {error_count}ä»¶")
        print(f"å®Œäº†æ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*70)

        return existing_data

    def monthly_update(self):
        """
        æœˆæ¬¡æ›´æ–°ï¼ˆéå»1ãƒ¶æœˆã®æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿åé›†ï¼‰
        """
        print("="*70)
        print("ğŸ“… æœˆæ¬¡æ›´æ–°ã‚’é–‹å§‹ã—ã¾ã™")
        print("="*70)

        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        if not self.master_db.exists():
            print("âŒ ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
            print("åˆå›åé›†ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„")
            return None

        with open(self.master_db, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)

        print(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_data)}ä»¶")

        # éå»30æ—¥åˆ†ã®ã¿åé›†
        end_date = datetime.now()
        start_date = end_date - timedelta(days=30)

        monthly_data = {}
        new_count = 0

        for idx, keyword in enumerate(self.all_keywords, 1):
            print(f"[{idx}/{len(self.all_keywords)}] {keyword}", end=" ")

            try:
                # æ–°è¦è«–æ–‡ã®ã¿æ¤œç´¢ï¼ˆæœ€å¤§20ä»¶ï¼‰
                papers = self.collector.search_and_fetch(
                    keyword,
                    max_results=20,
                    start_date=start_date,
                    end_date=end_date
                )

                if papers:
                    new_papers = 0
                    for paper in papers:
                        paper_id = self._generate_paper_id(paper)
                        if paper_id not in existing_data:
                            monthly_data[paper_id] = paper
                            monthly_data[paper_id]['keywords'] = [keyword]
                            monthly_data[paper_id]['collected_at'] = datetime.now().isoformat()
                            existing_data[paper_id] = monthly_data[paper_id]
                            new_papers += 1

                    new_count += new_papers
                    print(f"â†’ æ–°è¦{new_papers}ä»¶")
                else:
                    print("â†’ 0ä»¶")

                time.sleep(0.1)

            except Exception as e:
                print(f"â†’ ã‚¨ãƒ©ãƒ¼")
                continue

        # ä¿å­˜
        if monthly_data:
            # æœˆæ¬¡ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            monthly_file = self.monthly_dir / f"update_{datetime.now().strftime('%Y%m')}.json"
            with open(monthly_file, 'w', encoding='utf-8') as f:
                json.dump(monthly_data, f, ensure_ascii=False, indent=2)

            # ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°
            self._save_master_database(existing_data)

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿æ›´æ–°
            self._update_metadata({
                'last_monthly_update': datetime.now().isoformat(),
                'monthly_new_papers': new_count,
                'total_papers': len(existing_data)
            })

        print(f"\nâœ… æœˆæ¬¡æ›´æ–°å®Œäº†: æ–°è¦{new_count}ä»¶è¿½åŠ ")
        print(f"ç·è«–æ–‡æ•°: {len(existing_data)}ä»¶")

        return monthly_data

    def _generate_paper_id(self, paper):
        """è«–æ–‡ã®ä¸€æ„IDã‚’ç”Ÿæˆ"""
        # PMIDãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        if 'pmid' in paper:
            return f"pmid_{paper['pmid']}"
        # ãªã‘ã‚Œã°ã‚¿ã‚¤ãƒˆãƒ«ã¨è‘—è€…ã‹ã‚‰ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆ
        text = f"{paper.get('title', '')}{paper.get('authors', [])}"
        return hashlib.md5(text.encode()).hexdigest()

    def _save_master_database(self, data):
        """ãƒã‚¹ã‚¿ãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        with open(self.master_db, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

    def _update_metadata(self, updates):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        if self.meta_db.exists():
            with open(self.meta_db, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
        else:
            metadata = {}

        metadata.update(updates)

        with open(self.meta_db, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)

    def get_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        if not self.master_db.exists():
            return None

        with open(self.master_db, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å¹´åˆ¥çµ±è¨ˆ
        year_stats = {}
        keyword_stats = {}

        for paper_id, paper in data.items():
            # å¹´åˆ¥
            if 'publication_date' in paper:
                year = paper['publication_date'][:4]
                year_stats[year] = year_stats.get(year, 0) + 1

            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥
            for keyword in paper.get('keywords', []):
                keyword_stats[keyword] = keyword_stats.get(keyword, 0) + 1

        return {
            'total_papers': len(data),
            'year_distribution': year_stats,
            'keyword_distribution': keyword_stats,
            'database_size_mb': self.master_db.stat().st_size / (1024 * 1024)
        }


def main():
    parser = argparse.ArgumentParser(description='æœˆæ¬¡å¤§é‡ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--mode', choices=['initial', 'update', 'stats'],
                       default='stats', help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰')
    parser.add_argument('--years', type=int, default=10,
                       help='åˆå›åé›†ã®å¹´æ•°')
    parser.add_argument('--max-papers', type=int, default=100,
                       help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§è«–æ–‡æ•°')

    args = parser.parse_args()

    system = MonthlyCollectionSystem()

    if args.mode == 'initial':
        # åˆå›å¤§é‡åé›†
        system.initial_massive_collection(
            years=args.years,
            papers_per_keyword=args.max_papers
        )

    elif args.mode == 'update':
        # æœˆæ¬¡æ›´æ–°
        system.monthly_update()

    elif args.mode == 'stats':
        # çµ±è¨ˆè¡¨ç¤º
        stats = system.get_statistics()
        if stats:
            print("\nğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çµ±è¨ˆ")
            print("="*50)
            print(f"ç·è«–æ–‡æ•°: {stats['total_papers']:,}ä»¶")
            print(f"ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {stats['database_size_mb']:.1f} MB")
            print("\nå¹´åˆ¥åˆ†å¸ƒ:")
            for year in sorted(stats['year_distribution'].keys()):
                count = stats['year_distribution'][year]
                print(f"  {year}å¹´: {count:,}ä»¶")
            print("\nã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¥TOP10:")
            for keyword, count in sorted(stats['keyword_distribution'].items(),
                                        key=lambda x: x[1], reverse=True)[:10]:
                print(f"  {keyword}: {count}ä»¶")
        else:
            print("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒå­˜åœ¨ã—ã¾ã›ã‚“")


if __name__ == "__main__":
    main()