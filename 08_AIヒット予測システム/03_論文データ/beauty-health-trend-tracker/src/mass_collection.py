#!/usr/bin/env python3
"""
å¤§è¦æ¨¡è«–æ–‡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ç¾å®¹ãƒ»å¥åº·åˆ†é‡ã®è«–æ–‡ã‚’å¤§é‡ã«åé›†
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from dotenv import load_dotenv
import argparse

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.collectors.pubmed_collector import PubMedCollector

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# æ‹¡å¼µã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚»ãƒƒãƒˆï¼ˆç¾å®¹ãƒ»å¥åº·é–¢é€£ã®åŒ…æ‹¬çš„ãƒªã‚¹ãƒˆï¼‰
EXPANDED_KEYWORDS = {
    # æ—¢å­˜ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
    "core": [
        "NMN anti-aging",
        "collagen supplement skin",
        "hyaluronic acid hydration",
        "retinol skincare",
        "CBD inflammation",
        "exosome regeneration",
        "peptide anti-aging",
        "niacinamide brightening",
        "ceramide barrier",
        "vitamin C antioxidant"
    ],

    # ã‚¹ã‚­ãƒ³ã‚±ã‚¢æˆåˆ†
    "skincare_ingredients": [
        "retinoid wrinkle",
        "salicylic acid acne",
        "glycolic acid exfoliation",
        "lactic acid moisturizing",
        "vitamin E skin protection",
        "resveratrol antioxidant skin",
        "bakuchiol retinol alternative",
        "tranexamic acid melasma",
        "azelaic acid rosacea",
        "centella asiatica healing"
    ],

    # ã‚¢ãƒ³ãƒã‚¨ã‚¤ã‚¸ãƒ³ã‚°
    "anti_aging": [
        "NAD+ longevity",
        "sirtuin activation aging",
        "telomerase anti-aging",
        "growth factor skin rejuvenation",
        "stem cell cosmetics",
        "autophagy skin aging",
        "senolytic compounds",
        "mitochondrial dysfunction aging",
        "oxidative stress skin aging",
        "collagen synthesis stimulation"
    ],

    # ç¾å®¹ã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆ
    "beauty_supplements": [
        "biotin hair growth",
        "omega-3 skin health",
        "probiotics skin microbiome",
        "astaxanthin UV protection",
        "coenzyme Q10 wrinkle",
        "glutathione skin whitening",
        "zinc acne treatment",
        "vitamin D skin health",
        "curcumin anti-inflammatory skin",
        "green tea polyphenols skin"
    ],

    # å…ˆé€²çš„ãƒˆãƒªãƒ¼ãƒˆãƒ¡ãƒ³ãƒˆ
    "advanced_treatments": [
        "microneedling collagen induction",
        "LED therapy skin rejuvenation",
        "radiofrequency skin tightening",
        "platelet-rich plasma facial",
        "mesotherapy skin treatment",
        "cryotherapy anti-aging",
        "ultrasound skin therapy",
        "laser resurfacing wrinkle",
        "chemical peel rejuvenation",
        "dermabrasion skin texture"
    ],

    # å¤©ç„¶ãƒ»ã‚ªãƒ¼ã‚¬ãƒ‹ãƒƒã‚¯æˆåˆ†
    "natural_ingredients": [
        "aloe vera skin healing",
        "tea tree oil acne",
        "rosehip oil scar treatment",
        "jojoba oil moisturizer",
        "argan oil anti-aging",
        "manuka honey wound healing",
        "turmeric skin inflammation",
        "ginseng skin vitality",
        "licorice root pigmentation",
        "chamomile sensitive skin"
    ]
}

class MassDataCollector:
    """å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.collector = PubMedCollector(api_key=os.getenv('NCBI_API_KEY'))
        self.data_dir = Path('./data/mass_collection')
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def collect_batch(self, keywords, years_back=5, papers_per_keyword=50):
        """
        ãƒãƒƒãƒã§ãƒ‡ãƒ¼ã‚¿åé›†

        Args:
            keywords: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
            years_back: é¡ã‚‹å¹´æ•°
            papers_per_keyword: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§è«–æ–‡æ•°
        """
        all_results = {}
        total_papers = 0

        end_date = datetime(2024, 9, 24)
        start_date = end_date - timedelta(days=365 * years_back)

        print(f"\nğŸ“š ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        print(f"æœŸé–“: {start_date.strftime('%Y/%m/%d')} ã€œ {end_date.strftime('%Y/%m/%d')}")
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(keywords)}")
        print(f"æœ€å¤§è«–æ–‡æ•°/ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {papers_per_keyword}")
        print("="*60)

        for idx, keyword in enumerate(keywords, 1):
            print(f"\n[{idx}/{len(keywords)}] {keyword}")

            try:
                # è«–æ–‡æ¤œç´¢
                papers = self.collector.search_and_fetch(
                    keyword,
                    max_results=papers_per_keyword,
                    start_date=start_date,
                    end_date=end_date
                )

                if papers:
                    all_results[keyword] = papers
                    total_papers += len(papers)
                    print(f"  âœ… {len(papers)}ä»¶åé›†")
                else:
                    print(f"  âš ï¸ ãƒ‡ãƒ¼ã‚¿ãªã—")

                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
                time.sleep(0.5)

                # é€²æ—ä¿å­˜ï¼ˆ10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã”ã¨ï¼‰
                if idx % 10 == 0:
                    self._save_checkpoint(all_results, idx)

            except Exception as e:
                print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        return all_results, total_papers

    def _save_checkpoint(self, data, checkpoint_num):
        """ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
        checkpoint_file = self.data_dir / f"checkpoint_{checkpoint_num}.json"
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜: {checkpoint_file.name}")

    def save_results(self, data, total_papers):
        """çµæœã‚’ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')

        # ãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«
        output_file = self.data_dir / f"papers_mass_{timestamp}.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        # çµ±è¨ˆæƒ…å ±ãƒ•ã‚¡ã‚¤ãƒ«
        stats_file = self.data_dir / f"stats_{timestamp}.json"
        stats = {
            "timestamp": timestamp,
            "total_papers": total_papers,
            "total_keywords": len(data),
            "papers_per_keyword": {k: len(v) for k, v in data.items()},
            "date_range": self._get_date_range(data)
        }
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        return output_file, stats_file

    def _get_date_range(self, data):
        """ãƒ‡ãƒ¼ã‚¿ã®æ—¥ä»˜ç¯„å›²ã‚’å–å¾—"""
        all_dates = []
        for papers in data.values():
            for paper in papers:
                if 'publication_date' in paper:
                    all_dates.append(paper['publication_date'])

        if all_dates:
            all_dates.sort()
            return {
                "earliest": all_dates[0],
                "latest": all_dates[-1],
                "total_days": (datetime.fromisoformat(all_dates[-1]) -
                              datetime.fromisoformat(all_dates[0])).days
            }
        return None


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    parser = argparse.ArgumentParser(description='å¤§è¦æ¨¡è«–æ–‡ãƒ‡ãƒ¼ã‚¿åé›†')
    parser.add_argument('--category', default='core',
                       choices=list(EXPANDED_KEYWORDS.keys()) + ['all'],
                       help='åé›†ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ãƒ†ã‚´ãƒª')
    parser.add_argument('--years', type=int, default=5,
                       help='é¡ã‚‹å¹´æ•°')
    parser.add_argument('--max-papers', type=int, default=50,
                       help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§è«–æ–‡æ•°')

    args = parser.parse_args()

    # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é¸æŠ
    if args.category == 'all':
        keywords = []
        for category_keywords in EXPANDED_KEYWORDS.values():
            keywords.extend(category_keywords)
    else:
        keywords = EXPANDED_KEYWORDS[args.category]

    print("="*60)
    print("ğŸš€ å¤§è¦æ¨¡è«–æ–‡ãƒ‡ãƒ¼ã‚¿åé›†ã‚·ã‚¹ãƒ†ãƒ ")
    print("="*60)
    print(f"ã‚«ãƒ†ã‚´ãƒª: {args.category}")
    print(f"æœŸé–“: éå»{args.years}å¹´")
    print(f"äºˆæƒ³åé›†æ•°: æœ€å¤§{len(keywords) * args.max_papers}ä»¶")

    # åé›†å®Ÿè¡Œ
    collector = MassDataCollector()
    results, total = collector.collect_batch(
        keywords,
        years_back=args.years,
        papers_per_keyword=args.max_papers
    )

    # çµæœä¿å­˜
    if results:
        data_file, stats_file = collector.save_results(results, total)

        print("\n" + "="*60)
        print("âœ… åé›†å®Œäº†!")
        print(f"ç·è«–æ–‡æ•°: {total}ä»¶")
        print(f"æˆåŠŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {len(results)}/{len(keywords)}")
        print(f"ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«: {data_file}")
        print(f"çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«: {stats_file}")
        print("="*60)
    else:
        print("\nâŒ ãƒ‡ãƒ¼ã‚¿åé›†ã«å¤±æ•—ã—ã¾ã—ãŸ")


if __name__ == "__main__":
    main()