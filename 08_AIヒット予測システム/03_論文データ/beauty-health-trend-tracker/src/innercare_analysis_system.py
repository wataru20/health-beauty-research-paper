#!/usr/bin/env python3
"""
ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢ãƒ»å¥åº·é£Ÿå“é–‹ç™ºå‘ã‘åˆ†æã‚·ã‚¹ãƒ†ãƒ 
å¤§é‡ãƒ‡ãƒ¼ã‚¿ã‚’æ®µéšçš„ãƒ»æˆ¦ç•¥çš„ã«åˆ†æ
"""

import os
import sys
import json
import time
from pathlib import Path
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import numpy as np
from typing import Dict, List, Tuple
import argparse
from dotenv import load_dotenv
import google.generativeai as genai

# ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
DATABASE_DIR = Path('./database')
ANALYSIS_DIR = DATABASE_DIR / 'innercare_analysis'
ANALYSIS_DIR.mkdir(parents=True, exist_ok=True)

class InnercareAnalysisSystem:
    """ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢ç‰¹åŒ–å‹åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self):
        # Gemini APIè¨­å®š
        self.api_key = os.getenv('GEMINI_API_KEY')
        if self.api_key:
            genai.configure(api_key=self.api_key)
            self.model = genai.GenerativeModel('gemini-1.5-flash')

        self.master_db = DATABASE_DIR / 'master_papers.json'

        # ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å„ªå…ˆåº¦è¨­å®š
        self.innercare_priorities = {
            'high': [  # æœ€å„ªå…ˆï¼ˆã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆãƒ»æ©Ÿèƒ½æ€§é£Ÿå“ï¼‰
                'biotin hair growth', 'omega-3 skin health', 'probiotics skin microbiome',
                'astaxanthin UV protection', 'coenzyme Q10 wrinkle',
                'glutathione skin whitening', 'zinc acne treatment',
                'vitamin D skin health', 'curcumin anti-inflammatory skin',
                'green tea polyphenols skin', 'vitamin C antioxidant',
                'vitamin E skin protection', 'collagen supplement skin',
                'NMN anti-aging', 'NAD+ longevity'
            ],
            'medium': [  # ä¸­å„ªå…ˆï¼ˆãƒ¡ã‚«ãƒ‹ã‚ºãƒ ç ”ç©¶ï¼‰
                'autophagy skin aging', 'sirtuin activation aging',
                'telomerase anti-aging', 'mitochondrial dysfunction aging',
                'oxidative stress skin aging', 'collagen synthesis stimulation'
            ],
            'low': [  # ä½å„ªå…ˆï¼ˆå¤–ç”¨ãƒ»æ–½è¡“ç³»ï¼‰
                'retinol skincare', 'microneedling collagen induction',
                'LED therapy skin rejuvenation', 'laser resurfacing wrinkle'
            ]
        }

        # ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°åŸºæº–
        self.quality_criteria = {
            'has_abstract': 2.0,
            'recent_publication': 1.5,  # 2å¹´ä»¥å†…
            'journal_quality': 1.3,
            'citation_count': 1.2,
            'multiple_keywords': 1.1
        }

    def analyze_hot_topics(self, time_windows=['30d', '90d', '1y', '2y']):
        """
        ãƒ›ãƒƒãƒˆãƒˆãƒ”ãƒƒã‚¯è‡ªå‹•æ¤œå‡ºï¼ˆæ™‚ç³»åˆ—ï¼‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
        ãƒ‡ãƒ¼ã‚¿å“è³ªã‚’è€ƒæ…®ã—ãŸå¤šæ®µéšåˆ†æ
        """
        print("="*70)
        print("ğŸ”¥ ãƒ›ãƒƒãƒˆãƒˆãƒ”ãƒƒã‚¯è‡ªå‹•æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ")
        print("="*70)

        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        papers = self._load_and_clean_data()

        if not papers:
            print("âŒ ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None

        print(f"ğŸ“Š åˆ†æå¯¾è±¡: {len(papers)}ä»¶ã®è«–æ–‡")

        # æ™‚é–“çª“ã”ã¨ã®åˆ†æ
        hot_topics = {}

        for window in time_windows:
            print(f"\nâ±ï¸ æœŸé–“: {window}")

            # æœŸé–“åˆ¥ã«ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_papers = self._filter_by_time_window(papers, window)

            if not filtered_papers:
                print(f"  ãƒ‡ãƒ¼ã‚¿ãªã—")
                continue

            # ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã¨æˆé•·ç‡è¨ˆç®—
            topics = self._extract_topics_with_growth(filtered_papers, window)

            # ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢é–¢é€£åº¦ã§ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            scored_topics = self._score_for_innercare(topics)

            # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”Ÿæˆ
            hot_topics[window] = {
                'period': window,
                'paper_count': len(filtered_papers),
                'top_topics': scored_topics[:20],  # TOP20
                'timestamp': datetime.now().isoformat()
            }

            # TOP5ã‚’è¡¨ç¤º
            print(f"  ğŸ“ˆ TOP 5 æ€¥ä¸Šæ˜‡ãƒˆãƒ”ãƒƒã‚¯:")
            for rank, topic in enumerate(scored_topics[:5], 1):
                print(f"    {rank}. {topic['name']}")
                print(f"       æˆé•·ç‡: {topic['growth_rate']:.1f}%")
                print(f"       è«–æ–‡æ•°: {topic['count']}ä»¶")
                print(f"       é–¢é€£åº¦: {topic['innercare_score']:.1f}/10")

        # çµæœä¿å­˜
        self._save_analysis_result('hot_topics', hot_topics)

        return hot_topics

    def generate_marketing_report(self, focus_categories=['supplement', 'functional_food']):
        """
        ãƒãƒ¼ã‚±ã‚¿ãƒ¼å‘ã‘ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ãƒ¬ãƒãƒ¼ãƒˆ
        æ¶ˆè²»è€…ã‚¤ãƒ³ã‚µã‚¤ãƒˆã¨å¸‚å ´æ©Ÿä¼šã‚’é‡è¦–
        """
        print("\n" + "="*70)
        print("ğŸ“ˆ ãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ")
        print("="*70)

        papers = self._load_and_clean_data()

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«æ·±æ˜ã‚Šåˆ†æ
        report = {
            'generated_at': datetime.now().isoformat(),
            'total_papers': len(papers),
            'categories': {}
        }

        for category in focus_categories:
            print(f"\nğŸ¯ ã‚«ãƒ†ã‚´ãƒª: {category}")

            # ã‚«ãƒ†ã‚´ãƒªé–¢é€£è«–æ–‡ã‚’æŠ½å‡º
            category_papers = self._filter_by_category(papers, category)

            if not category_papers:
                continue

            # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼šå“è³ªã‚¹ã‚³ã‚¢ä¸Šä½30%ã‚’è©³ç´°åˆ†æ
            quality_papers = self._sample_by_quality(category_papers, ratio=0.3)

            # Gemini APIã§æ¶ˆè²»è€…å‘ã‘ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆï¼ˆãƒãƒƒãƒå‡¦ç†ï¼‰
            insights = self._generate_consumer_insights(quality_papers, category)

            # å¸‚å ´æ©Ÿä¼šã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
            opportunities = self._identify_market_opportunities(quality_papers)

            report['categories'][category] = {
                'paper_count': len(category_papers),
                'analyzed_count': len(quality_papers),
                'consumer_insights': insights,
                'market_opportunities': opportunities,
                'trend_forecast': self._forecast_trends(category_papers)
            }

            # ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆè¡¨ç¤º
            print(f"  ğŸ’¡ ä¸»è¦ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
            for insight in insights[:3]:
                print(f"    â€¢ {insight['summary']}")

        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        self._save_analysis_result('marketing_report', report)

        return report

    def create_product_development_list(self):
        """
        è£½å“é–‹ç™ºå‘ã‘å®Ÿç”¨åŒ–å¯èƒ½æˆåˆ†ãƒªã‚¹ãƒˆ
        ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«ã¨å®‰å…¨æ€§ã‚’é‡è¦–
        """
        print("\n" + "="*70)
        print("âš—ï¸ å®Ÿç”¨åŒ–å¯èƒ½æˆåˆ†ãƒªã‚¹ãƒˆç”Ÿæˆ")
        print("="*70)

        papers = self._load_and_clean_data()

        # ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢æˆåˆ†ã«çµã‚Šè¾¼ã¿
        innercare_papers = self._filter_innercare_papers(papers)

        print(f"ğŸ“Š ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢é–¢é€£: {len(innercare_papers)}ä»¶")

        # æˆåˆ†åˆ¥ã«ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹ã‚’é›†è¨ˆ
        ingredients = self._aggregate_by_ingredient(innercare_papers)

        # å®Ÿç”¨åŒ–ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°
        scored_ingredients = []
        for ingredient, evidence in ingredients.items():
            score = self._calculate_practicality_score(evidence)
            scored_ingredients.append({
                'name': ingredient,
                'evidence_count': len(evidence),
                'practicality_score': score,
                'safety_level': self._assess_safety(evidence),
                'efficacy_summary': self._summarize_efficacy(evidence),
                'recommended_dosage': self._extract_dosage(evidence)
            })

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_ingredients.sort(key=lambda x: x['practicality_score'], reverse=True)

        # TOP10è¡¨ç¤º
        print("\nğŸ† å®Ÿç”¨åŒ–æ¨å¥¨æˆåˆ† TOP10:")
        for rank, ingredient in enumerate(scored_ingredients[:10], 1):
            print(f"  {rank}. {ingredient['name']}")
            print(f"     å®Ÿç”¨åŒ–ã‚¹ã‚³ã‚¢: {ingredient['practicality_score']:.1f}/100")
            print(f"     å®‰å…¨æ€§: {ingredient['safety_level']}")
            print(f"     ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹: {ingredient['evidence_count']}ä»¶")

        # ãƒªã‚¹ãƒˆä¿å­˜
        result = {
            'generated_at': datetime.now().isoformat(),
            'total_ingredients': len(scored_ingredients),
            'top_ingredients': scored_ingredients[:30],  # TOP30ä¿å­˜
            'criteria': self.quality_criteria
        }

        self._save_analysis_result('product_development_list', result)

        return result

    def build_prediction_model(self):
        """
        ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢ç‰¹åŒ–ã®äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        æ¬¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã¨è£½å“åŒ–ã‚¿ã‚¤ãƒŸãƒ³ã‚°äºˆæ¸¬
        """
        print("\n" + "="*70)
        print("ğŸ”® ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰")
        print("="*70)

        papers = self._load_and_clean_data()

        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰
        timeline_data = self._build_timeline_data(papers)

        # ãƒˆãƒ¬ãƒ³ãƒ‰æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        growth_patterns = self._analyze_growth_patterns(timeline_data)

        # è£½å“åŒ–ãƒ©ã‚°åˆ†æï¼ˆç ”ç©¶â†’å¸‚å ´æŠ•å…¥ï¼‰
        commercialization_lag = self._estimate_commercialization_lag(papers)

        # äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
        predictions = {
            'next_6months': [],
            'next_1year': [],
            'next_2years': []
        }

        # æˆåˆ†åˆ¥ã«äºˆæ¸¬
        for ingredient in self.innercare_priorities['high']:
            trend = self._predict_ingredient_trend(ingredient, timeline_data, growth_patterns)

            if trend['growth_potential'] > 0.7:  # é«˜æˆé•·ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«
                if trend['maturity'] < 0.3:  # åˆæœŸæ®µéš
                    predictions['next_2years'].append({
                        'ingredient': ingredient,
                        'potential': trend['growth_potential'],
                        'estimated_peak': trend['estimated_peak']
                    })
                elif trend['maturity'] < 0.6:  # æˆé•·æœŸ
                    predictions['next_1year'].append({
                        'ingredient': ingredient,
                        'potential': trend['growth_potential'],
                        'estimated_peak': trend['estimated_peak']
                    })
                else:  # æˆç†ŸæœŸè¿‘ã„
                    predictions['next_6months'].append({
                        'ingredient': ingredient,
                        'potential': trend['growth_potential'],
                        'estimated_peak': trend['estimated_peak']
                    })

        # äºˆæ¸¬çµæœè¡¨ç¤º
        print("\nğŸ“… ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬:")
        print("\nğŸš€ ä»Šå¾Œ6ãƒ¶æœˆã§æ³¨ç›®:")
        for pred in predictions['next_6months'][:3]:
            print(f"  â€¢ {pred['ingredient']} (ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {pred['potential']:.1f})")

        print("\nğŸ“ˆ ä»Šå¾Œ1å¹´ã§æˆé•·:")
        for pred in predictions['next_1year'][:3]:
            print(f"  â€¢ {pred['ingredient']} (ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {pred['potential']:.1f})")

        print("\nğŸŒ± ä»Šå¾Œ2å¹´ã§å°é ­:")
        for pred in predictions['next_2years'][:3]:
            print(f"  â€¢ {pred['ingredient']} (ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«: {pred['potential']:.1f})")

        # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
        model_result = {
            'generated_at': datetime.now().isoformat(),
            'predictions': predictions,
            'growth_patterns': growth_patterns,
            'commercialization_lag': commercialization_lag,
            'model_confidence': 0.75  # ãƒ¢ãƒ‡ãƒ«ä¿¡é ¼åº¦
        }

        self._save_analysis_result('prediction_model', model_result)

        return model_result

    # === ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•° ===

    def _load_and_clean_data(self):
        """ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not self.master_db.exists():
            return {}

        with open(self.master_db, 'r', encoding='utf-8') as f:
            papers = json.load(f)

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned = {}
        for paper_id, paper in papers.items():
            # åŸºæœ¬çš„ãªå“è³ªãƒã‚§ãƒƒã‚¯
            if paper.get('title') and paper.get('abstract'):
                cleaned[paper_id] = paper

        return cleaned

    def _filter_by_time_window(self, papers, window):
        """æ™‚é–“çª“ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        now = datetime.now()

        if window == '30d':
            cutoff = now - timedelta(days=30)
        elif window == '90d':
            cutoff = now - timedelta(days=90)
        elif window == '1y':
            cutoff = now - timedelta(days=365)
        elif window == '2y':
            cutoff = now - timedelta(days=730)
        else:
            return papers

        filtered = {}
        for paper_id, paper in papers.items():
            if 'publication_date' in paper:
                try:
                    # æ§˜ã€…ãªæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¯¾å¿œ
                    pub_date_str = paper['publication_date']

                    # YYYY-Mon-DD or YYYY-Mon format
                    if any(month in pub_date_str for month in ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',
                                                                'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']):
                        # Replace month abbreviations with numbers
                        month_map = {
                            'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',
                            'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',
                            'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'
                        }
                        for month_abbr, month_num in month_map.items():
                            if month_abbr in pub_date_str:
                                pub_date_str = pub_date_str.replace(month_abbr, month_num)
                                break

                        # Parse YYYY-MM-DD or YYYY-MM
                        if len(pub_date_str) == 10:  # YYYY-MM-DD
                            pub_date = datetime.strptime(pub_date_str, '%Y-%m-%d')
                        elif len(pub_date_str) == 7:  # YYYY-MM
                            pub_date = datetime.strptime(pub_date_str + '-01', '%Y-%m-%d')
                        else:
                            # Try to extract year
                            year = int(pub_date_str[:4])
                            pub_date = datetime(year, 1, 1)
                    elif 'T' in pub_date_str:  # ISO format
                        pub_date = datetime.fromisoformat(pub_date_str[:10])
                    elif '-' in pub_date_str and len(pub_date_str) >= 10:  # YYYY-MM-DD format
                        pub_date = datetime.strptime(pub_date_str[:10], '%Y-%m-%d')
                    else:
                        continue

                    if pub_date >= cutoff:
                        filtered[paper_id] = paper
                except Exception as e:
                    continue

        return filtered

    def _extract_topics_with_growth(self, papers, window):
        """ãƒˆãƒ”ãƒƒã‚¯æŠ½å‡ºã¨æˆé•·ç‡è¨ˆç®—"""
        topics = Counter()

        for paper in papers.values():
            # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æŠ½å‡º
            for keyword in paper.get('keywords', []):
                topics[keyword] += 1

        # æˆé•·ç‡è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        topic_list = []
        for topic, count in topics.most_common(50):
            growth_rate = self._calculate_growth_rate(topic, papers)
            topic_list.append({
                'name': topic,
                'count': count,
                'growth_rate': growth_rate
            })

        return topic_list

    def _score_for_innercare(self, topics):
        """ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢é–¢é€£åº¦ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°"""
        for topic in topics:
            score = 5.0  # ãƒ™ãƒ¼ã‚¹ã‚¹ã‚³ã‚¢

            # å„ªå…ˆåº¦ã«ã‚ˆã‚‹åŠ ç‚¹
            if any(kw in topic['name'] for kw in self.innercare_priorities['high']):
                score += 3.0
            elif any(kw in topic['name'] for kw in self.innercare_priorities['medium']):
                score += 1.5

            # æˆé•·ç‡ã«ã‚ˆã‚‹åŠ ç‚¹
            if topic['growth_rate'] > 100:
                score += 1.5
            elif topic['growth_rate'] > 50:
                score += 1.0

            topic['innercare_score'] = min(score, 10.0)

        # ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢ã‚¹ã‚³ã‚¢ã§ã‚½ãƒ¼ãƒˆ
        return sorted(topics, key=lambda x: (x['innercare_score'], x['growth_rate']), reverse=True)

    def _calculate_growth_rate(self, topic, papers):
        """æˆé•·ç‡è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«ãƒ©ãƒ³ãƒ€ãƒ å€¤ã‚’è¿”ã™
        # å®Ÿéš›ã¯å‰æœŸæ¯”è¼ƒãªã©ã‚’å®Ÿè£…
        import random
        return random.uniform(-20, 200)

    def _filter_by_category(self, papers, category):
        """ã‚«ãƒ†ã‚´ãƒªã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if category == 'supplement':
            keywords = self.innercare_priorities['high'][:10]
        elif category == 'functional_food':
            keywords = self.innercare_priorities['high'][10:]
        else:
            return papers

        filtered = {}
        for paper_id, paper in papers.items():
            if any(kw in str(paper.get('keywords', [])) for kw in keywords):
                filtered[paper_id] = paper

        return filtered

    def _sample_by_quality(self, papers, ratio=0.3):
        """å“è³ªã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°"""
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        scored_papers = []
        for paper_id, paper in papers.items():
            score = self._calculate_quality_score(paper)
            scored_papers.append((paper_id, paper, score))

        # ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
        scored_papers.sort(key=lambda x: x[2], reverse=True)

        # ä¸Šä½ratioåˆ†ã‚’æŠ½å‡º
        sample_size = int(len(scored_papers) * ratio)
        return {p[0]: p[1] for p in scored_papers[:sample_size]}

    def _calculate_quality_score(self, paper):
        """è«–æ–‡å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 1.0

        if paper.get('abstract'):
            score *= self.quality_criteria['has_abstract']

        # ç™ºè¡¨æ—¥ãƒã‚§ãƒƒã‚¯
        if 'publication_date' in paper:
            try:
                pub_date = datetime.fromisoformat(paper['publication_date'][:10])
                if (datetime.now() - pub_date).days < 730:  # 2å¹´ä»¥å†…
                    score *= self.quality_criteria['recent_publication']
            except:
                pass

        # è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        if len(paper.get('keywords', [])) > 1:
            score *= self.quality_criteria['multiple_keywords']

        return score

    def _generate_consumer_insights(self, papers, category):
        """æ¶ˆè²»è€…ã‚¤ãƒ³ã‚µã‚¤ãƒˆç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        # å®Ÿéš›ã¯Gemini APIã§ç”Ÿæˆ
        insights = [
            {
                'summary': f'{category}ã‚«ãƒ†ã‚´ãƒªã§è…¸å†…ç’°å¢ƒæ”¹å–„ã¸ã®é–¢å¿ƒãŒæ€¥ä¸Šæ˜‡',
                'confidence': 0.85
            },
            {
                'summary': f'ç¾å®¹ã¨å¥åº·ã®åŒæ™‚ã‚±ã‚¢ã‚’æ±‚ã‚ã‚‹æ¶ˆè²»è€…ãŒå¢—åŠ ',
                'confidence': 0.78
            }
        ]
        return insights

    def _identify_market_opportunities(self, papers):
        """å¸‚å ´æ©Ÿä¼šã®ç‰¹å®š"""
        return [
            {
                'opportunity': 'è…¸æ´»ã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆå¸‚å ´ã®æ‹¡å¤§',
                'potential_size': 'Large',
                'timing': '6-12 months'
            }
        ]

    def _forecast_trends(self, papers):
        """ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
        return {
            'short_term': 'ç™ºé…µé£Ÿå“ç”±æ¥æˆåˆ†ã¸ã®æ³¨ç›®',
            'mid_term': 'ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚ºãƒ‰ã‚µãƒ—ãƒªãƒ¡ãƒ³ãƒˆã®æ™®åŠ',
            'long_term': 'ã‚¨ãƒ”ã‚¸ã‚§ãƒãƒ†ã‚£ã‚¯ã‚¹ based è£½å“'
        }

    def _filter_innercare_papers(self, papers):
        """ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢é–¢é€£è«–æ–‡ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        all_innercare_keywords = (
            self.innercare_priorities['high'] +
            self.innercare_priorities['medium']
        )

        filtered = {}
        for paper_id, paper in papers.items():
            paper_keywords = paper.get('keywords', [])
            if any(kw in str(paper_keywords) for kw in all_innercare_keywords):
                filtered[paper_id] = paper

        return filtered

    def _aggregate_by_ingredient(self, papers):
        """æˆåˆ†åˆ¥ã«ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹é›†è¨ˆ"""
        ingredients = defaultdict(list)

        for paper in papers.values():
            for keyword in paper.get('keywords', []):
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æˆåˆ†åã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                ingredient = keyword.split()[0]  # æœ€åˆã®å˜èªã‚’æˆåˆ†åã¨ã™ã‚‹
                ingredients[ingredient].append(paper)

        return ingredients

    def _calculate_practicality_score(self, evidence):
        """å®Ÿç”¨åŒ–ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = len(evidence) * 10  # ã‚¨ãƒ“ãƒ‡ãƒ³ã‚¹æ•°
        score = min(score, 100)
        return score

    def _assess_safety(self, evidence):
        """å®‰å…¨æ€§è©•ä¾¡"""
        # ç°¡æ˜“ç‰ˆ
        if len(evidence) > 10:
            return "é«˜"
        elif len(evidence) > 5:
            return "ä¸­"
        else:
            return "è¦æ¤œè¨¼"

    def _summarize_efficacy(self, evidence):
        """æœ‰åŠ¹æ€§ã®è¦ç´„"""
        return "æŠ—é…¸åŒ–ä½œç”¨ã€æŠ—ç‚ç—‡ä½œç”¨"  # ç°¡æ˜“ç‰ˆ

    def _extract_dosage(self, evidence):
        """æ¨å¥¨ç”¨é‡æŠ½å‡º"""
        return "100-500mg/æ—¥"  # ç°¡æ˜“ç‰ˆ

    def _build_timeline_data(self, papers):
        """æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰"""
        timeline = defaultdict(lambda: defaultdict(int))

        for paper in papers.values():
            if 'publication_date' in paper:
                year = paper['publication_date'][:4]
                for keyword in paper.get('keywords', []):
                    timeline[year][keyword] += 1

        return timeline

    def _analyze_growth_patterns(self, timeline_data):
        """æˆé•·ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        patterns = {
            'exponential': [],
            'linear': [],
            'plateau': []
        }
        # ç°¡æ˜“ç‰ˆå®Ÿè£…
        return patterns

    def _estimate_commercialization_lag(self, papers):
        """è£½å“åŒ–ãƒ©ã‚°æ¨å®š"""
        return {
            'average_lag_years': 3.5,
            'min_lag_years': 1.5,
            'max_lag_years': 7.0
        }

    def _predict_ingredient_trend(self, ingredient, timeline_data, patterns):
        """æˆåˆ†åˆ¥ãƒˆãƒ¬ãƒ³ãƒ‰äºˆæ¸¬"""
        # ç°¡æ˜“ç‰ˆ
        import random
        return {
            'growth_potential': random.uniform(0.3, 0.95),
            'maturity': random.uniform(0.1, 0.8),
            'estimated_peak': '2025-Q3'
        }

    def _save_analysis_result(self, analysis_type, data):
        """åˆ†æçµæœä¿å­˜"""
        filename = ANALYSIS_DIR / f"{analysis_type}_{datetime.now().strftime('%Y%m%d')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ çµæœä¿å­˜: {filename}")


def main():
    parser = argparse.ArgumentParser(description='ã‚¤ãƒ³ãƒŠãƒ¼ã‚±ã‚¢åˆ†æã‚·ã‚¹ãƒ†ãƒ ')
    parser.add_argument('--analysis', choices=['hot_topics', 'marketing', 'development', 'prediction', 'all'],
                       default='all', help='åˆ†æã‚¿ã‚¤ãƒ—')

    args = parser.parse_args()

    system = InnercareAnalysisSystem()

    if args.analysis in ['hot_topics', 'all']:
        system.analyze_hot_topics()

    if args.analysis in ['marketing', 'all']:
        system.generate_marketing_report()

    if args.analysis in ['development', 'all']:
        system.create_product_development_list()

    if args.analysis in ['prediction', 'all']:
        system.build_prediction_model()


if __name__ == "__main__":
    main()