#!/usr/bin/env python3
"""
ç¾å®¹ãƒ»å¥åº·æˆåˆ†ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ  - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import shutil

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

from src.collectors.pubmed_collector import PubMedCollector
from src.analyzers.paper_summarizer import PaperSummarizer


class TrendTracker:
    """ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        """åˆæœŸåŒ–"""
        self.project_root = Path(__file__).parent.parent
        self.config_path = self.project_root / "configs" / "keywords.json"
        self.data_dir = self.project_root / "data"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        (self.data_dir / "raw").mkdir(parents=True, exist_ok=True)
        (self.data_dir / "processed").mkdir(parents=True, exist_ok=True)
        (self.data_dir / "trends").mkdir(parents=True, exist_ok=True)
        
        # è¨­å®šèª­ã¿è¾¼ã¿
        self.config = self._load_config()
        
        # API ã‚­ãƒ¼å–å¾—
        self.ncbi_api_key = os.environ.get('NCBI_API_KEY')
        self.gemini_api_key = os.environ.get('GEMINI_API_KEY')
    
    def _load_config(self) -> dict:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def _get_all_keywords(self) -> list:
        """å…¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒªã‚¹ãƒˆå½¢å¼ã§å–å¾—"""
        keywords = []
        
        # ãƒ‹ãƒ¼ã‚ºãƒ»æ‚©ã¿è»¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for category in self.config['categories'].values():
            for subcategory in category['subcategories'].values():
                keywords.extend(subcategory['keywords'])
        
        # æˆåˆ†è»¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰  
        for ingredient_group in self.config['ingredients'].values():
            if isinstance(ingredient_group, dict) and 'subcategories' in ingredient_group:
                for subcategory in ingredient_group['subcategories'].values():
                    keywords.extend(subcategory['keywords'])
            elif isinstance(ingredient_group, dict) and 'keywords' in ingredient_group:
                keywords.extend(ingredient_group['keywords'])
        
        # ã‚³ãƒ³ã‚»ãƒ—ãƒˆè»¸ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        for concept_group in self.config['concepts'].values():
            if 'keywords' in concept_group:
                keywords.extend(concept_group['keywords'])
        
        return list(set(keywords))  # é‡è¤‡é™¤å»
    
    def collect_papers(self, days_back: int = 30, max_papers: int = 10):
        """
        è«–æ–‡ã‚’åé›†
        
        Args:
            days_back: ä½•æ—¥å‰ã¾ã§ã®è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ã‹
            max_papers: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§å–å¾—ä»¶æ•°
        """
        print("=" * 50)
        print("ğŸ“š è«–æ–‡åé›†ã‚’é–‹å§‹ã—ã¾ã™")
        print("=" * 50)
        
        # ã‚³ãƒ¬ã‚¯ã‚¿ãƒ¼åˆæœŸåŒ–
        collector = PubMedCollector(api_key=self.ncbi_api_key)
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å–å¾—ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        all_keywords = self._get_all_keywords()
        
        # å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¨­å®šï¼ˆé‡è¦ãªæˆåˆ†ã‚’å„ªå…ˆï¼‰
        priority_keywords = [
            "NMN", "collagen", "hyaluronic acid", "retinol", "CBD",
            "exosome", "stem cell", "peptide", "ceramide", "niacinamide"
        ]
        
        # è‹±èªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«å¤‰æ›ï¼ˆç°¡æ˜“ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰
        keyword_mapping = {
            "NMN": "NMN anti-aging",
            "ã‚³ãƒ©ãƒ¼ã‚²ãƒ³": "collagen supplement skin",
            "ãƒ’ã‚¢ãƒ«ãƒ­ãƒ³é…¸": "hyaluronic acid skin hydration",
            "ãƒ¬ãƒãƒãƒ¼ãƒ«": "retinol anti-aging skincare",
            "CBD": "CBD skincare inflammation",
            "ã‚¨ã‚¯ã‚½ã‚½ãƒ¼ãƒ ": "exosome skin regeneration",
            "ãƒ’ãƒˆå¹¹ç´°èƒ": "stem cell cosmetics",
            "ãƒšãƒ—ãƒãƒ‰": "peptide anti-aging skin",
            "ã‚»ãƒ©ãƒŸãƒ‰": "ceramide skin barrier",
            "ãƒŠã‚¤ã‚¢ã‚·ãƒ³ã‚¢ãƒŸãƒ‰": "niacinamide skin brightening"
        }
        
        # æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’é¸æŠï¼ˆå„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ + ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒ«ï¼‰
        search_keywords = priority_keywords[:5]  # ç„¡æ–™æ ã‚’è€ƒæ…®ã—ã¦åˆ¶é™
        
        print(f"æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ•°: {len(search_keywords)}")
        print(f"æœŸé–“: éå»{days_back}æ—¥é–“")
        print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šæœ€å¤§: {max_papers}ä»¶\n")
        
        # è«–æ–‡åé›†
        results = collector.collect_papers_for_keywords(
            search_keywords,
            max_per_keyword=max_papers,
            days_back=days_back
        )
        
        # çµæœã‚’ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_path = self.data_dir / "raw" / f"papers_{timestamp}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… åé›†å®Œäº†: {output_path}")
        
        # çµ±è¨ˆè¡¨ç¤º
        total_papers = sum(len(papers) for papers in results.values())
        print(f"ç·è«–æ–‡æ•°: {total_papers}ä»¶")
        
        return output_path
    
    def analyze_papers(self):
        """è«–æ–‡ã‚’åˆ†æãƒ»è¦ç´„"""
        print("=" * 50)
        print("ğŸ”¬ è«–æ–‡åˆ†æã‚’é–‹å§‹ã—ã¾ã™")
        print("=" * 50)
        
        if not self.gemini_api_key:
            print("âš ï¸ Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
            print("ç’°å¢ƒå¤‰æ•° GEMINI_API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„")
            return None
        
        # æœ€æ–°ã®ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        raw_files = sorted(
            (self.data_dir / "raw").glob("papers_*.json"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )
        
        if not raw_files:
            print("âŒ åˆ†æã™ã‚‹è«–æ–‡ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return None
        
        latest_file = raw_files[0]
        print(f"åˆ†æå¯¾è±¡: {latest_file.name}")
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        with open(latest_file, 'r', encoding='utf-8') as f:
            papers_data = json.load(f)
        
        # è¦ç´„å™¨åˆæœŸåŒ–
        summarizer = PaperSummarizer(api_key=self.gemini_api_key)
        
        # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®è«–æ–‡ã‚’è¦ç´„
        summarized_data = {}
        total_papers = 0
        
        for keyword, papers in papers_data.items():
            if papers:
                print(f"\nè¦ç´„ä¸­: {keyword}")
                # ç„¡æ–™æ ã‚’è€ƒæ…®ã—ã¦æœ€å¤§3ä»¶ã¾ã§
                summarized = summarizer.batch_summarize(papers, max_papers=3)
                summarized_data[keyword] = summarized
                total_papers += len(summarized)
        
        # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        processed_path = self.data_dir / "processed" / f"summarized_{timestamp}.json"
        
        with open(processed_path, 'w', encoding='utf-8') as f:
            json.dump(summarized_data, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… è¦ç´„å®Œäº†: {processed_path}")
        
        # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
        print("\nğŸ“Š ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æä¸­...")
        trend_analysis = summarizer.analyze_trends(summarized_data)
        
        # åˆ†æçµæœã‚’ä¿å­˜
        analysis_path = self.data_dir / "trends" / f"analysis_{timestamp}.json"
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(trend_analysis, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†æå®Œäº†: {analysis_path}")
        
        # ã‚¤ãƒ³ã‚µã‚¤ãƒˆè¡¨ç¤º
        print("\nğŸ’¡ ãƒˆãƒ¬ãƒ³ãƒ‰ã‚¤ãƒ³ã‚µã‚¤ãƒˆ:")
        for insight in trend_analysis.get('trend_insights', []):
            print(f"  â€¢ {insight}")
        
        return analysis_path
    
    def prepare_dashboard_data(self):
        """ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™"""
        print("=" * 50)
        print("ğŸ“Š ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ã—ã¾ã™")
        print("=" * 50)
        
        # æœ€æ–°ã®åˆ†æçµæœã‚’å–å¾—
        trend_files = sorted(
            (self.data_dir / "trends").glob("analysis_*.json"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )
        
        processed_files = sorted(
            (self.data_dir / "processed").glob("summarized_*.json"),
            key=lambda x: x.stat().st_mtime,
            reverse=True
        )
        
        if not trend_files or not processed_files:
            print("âŒ å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return False
        
        # æœ€æ–°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼
        latest_analysis = trend_files[0]
        latest_papers = processed_files[0]
        
        # ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«åã§ã‚³ãƒ”ãƒ¼
        shutil.copy(
            latest_analysis,
            self.data_dir / "trends" / "latest_analysis.json"
        )
        
        shutil.copy(
            latest_papers,
            self.data_dir / "processed" / "latest_papers.json"
        )
        
        print("âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™å®Œäº†")
        print(f"  åˆ†æ: {latest_analysis.name}")
        print(f"  è«–æ–‡: {latest_papers.name}")
        
        return True
    
    def run(self, mode: str, **kwargs):
        """
        ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
        
        Args:
            mode: å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼ˆcollect, analyze, dashboard, allï¼‰
        """
        if mode in ['collect', 'all']:
            self.collect_papers(
                days_back=kwargs.get('days_back', 30),
                max_papers=kwargs.get('max_papers', 10)
            )
        
        if mode in ['analyze', 'all']:
            self.analyze_papers()
        
        if mode in ['dashboard', 'all']:
            self.prepare_dashboard_data()


def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    parser = argparse.ArgumentParser(
        description='ç¾å®¹ãƒ»å¥åº·æˆåˆ†ãƒˆãƒ¬ãƒ³ãƒ‰è¿½è·¡ã‚·ã‚¹ãƒ†ãƒ '
    )
    
    parser.add_argument(
        '--mode',
        choices=['collect', 'analyze', 'dashboard', 'all'],
        default='all',
        help='å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰'
    )
    
    parser.add_argument(
        '--days-back',
        type=int,
        default=30,
        help='ä½•æ—¥å‰ã¾ã§ã®è«–æ–‡ã‚’æ¤œç´¢ã™ã‚‹ã‹'
    )
    
    parser.add_argument(
        '--max-papers',
        type=int,
        default=10,
        help='ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚ãŸã‚Šã®æœ€å¤§å–å¾—ä»¶æ•°'
    )
    
    args = parser.parse_args()
    
    # ã‚·ã‚¹ãƒ†ãƒ å®Ÿè¡Œ
    tracker = TrendTracker()
    tracker.run(
        mode=args.mode,
        days_back=args.days_back,
        max_papers=args.max_papers
    )


if __name__ == "__main__":
    main()
